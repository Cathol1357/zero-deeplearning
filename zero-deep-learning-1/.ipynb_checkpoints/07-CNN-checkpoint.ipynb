{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a3e6159-f9fa-4640-9b89-1fd9456ccf7b",
   "metadata": {},
   "source": [
    "# 第7章 畳み込みニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e5ea42-5429-4802-b13d-28d308edc08e",
   "metadata": {},
   "source": [
    "* ## 7.4 Convolution / Poolingレイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b9081-5bdd-4b52-8f07-cbb50bad26ac",
   "metadata": {},
   "source": [
    "#### CNNでは、各層を流れるデータは四次元のデータになる。例えば、データの形状が(10, 1, 28, 28)だと、高さ28横幅28、1チャンネルのデータが10個存在しているのと同じ。\n",
    "#### 普通にfor文で実装するとえらい時間がかかるので、im2colを用いてデータを展開することで、高速に実装する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3e204c-0a46-425a-99e7-1ea087f5ccf8",
   "metadata": {},
   "source": [
    "* ## im2col関数の引数\n",
    "### im2col(input_data, filter_h, filter_w, stride=1, pad=0)\n",
    "\n",
    "#### input_data : (データ数、チャンネル、高さ、横幅）の四次元配列からなる入力データ\n",
    "#### filter_h : フィルターの高さ\n",
    "#### filter_w : フィルターの横幅\n",
    "#### stride : ストライド\n",
    "#### pad : パディング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63001553-13a1-4ee1-9659-0ce0e284b0be",
   "metadata": {},
   "source": [
    "* ## Convolutionレイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cc683b6-9339-448d-9bdc-c789e20de91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n"
     ]
    }
   ],
   "source": [
    "# im2colの動作確認\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.util import im2col\n",
    "\n",
    "# データ数1, ３チャンネル、 高さと横幅7のデータを用意\n",
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "821cec3b-6890-41b3-932a-8655946d484b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "# バッチサイズ（データサイズ）を１０倍にする\n",
    "# データサイズが10倍なので縦（行）要素も10倍になった。\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a9954d5-4a3e-4259-ad7c-c5ea55e0fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 畳み込み層\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape # フィルター\n",
    "        N, C, H, W = x.shape # データ\n",
    "        out_h = int(1 + (H + 2*self.pad -FH) / self.stride) # 出力データの高さ\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride) # 出力データの横幅\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        # reshapeの引数に-1を渡すといい感じに整形してくれる\n",
    "        col_W = self.W.reshape(FN, -1).T # フィルターの展開\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "\n",
    "        # transposeで軸の順番を入れ替える\n",
    "        # 変更前は（Num, Height, Width, Channel）の順なので、入力データと同様に\n",
    "        # （Num, Channel, Height, Wirth）の順番に入れ替える\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b292f-9c2a-41f9-a8ff-9b6146c68e5f",
   "metadata": {},
   "source": [
    "* ## Poolingレイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2b16420-745c-4502-8a34-efb50cbc2d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=2, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride) # 出力データの高さ\n",
    "        out_w = int(1 + (W + self.pool_w) / self.stride) # 出力データの横幅\n",
    "\n",
    "        # （１）展開\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "\n",
    "        # （２）最大値を抜き出す\n",
    "        out = np.max(col, axis=1)\n",
    "        # （３）整形\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7de7bed-428f-4707-83e9-6354f20b70a9",
   "metadata": {},
   "source": [
    "* # 7.5 CNNの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60eb26e1-e966-4607-8519-50d2ec362213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"単純なConvNet\n",
    "\n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 入力サイズ（MNISTの場合は784）\n",
    "    hidden_size_list : 隠れ層のニューロンの数のリスト（e.g. [100, 100, 100]）\n",
    "    output_size : 出力サイズ（MNISTの場合は10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
    "        'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
    "        'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"損失関数を求める\n",
    "        引数のxは入力データ、tは教師ラベル\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（数値微分）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（誤差逆伝搬法）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ce91b2-0d7a-465c-a056-050f9742e30d",
   "metadata": {},
   "source": [
    "* # MNISTデータセットに対して学習と予測を行ってみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9864409e-30c3-4200-b330-ea30f1ee5244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.298877533975644\n",
      "=== epoch:1, train acc:0.257, test acc:0.261 ===\n",
      "train loss:2.295309830897032\n",
      "train loss:2.2926989171859256\n",
      "train loss:2.2841202581873197\n",
      "train loss:2.276343338438866\n",
      "train loss:2.2419306679599416\n",
      "train loss:2.2468742791144267\n",
      "train loss:2.1872279172456723\n",
      "train loss:2.195035269637235\n",
      "train loss:2.1748456232448587\n",
      "train loss:2.1566523538070914\n",
      "train loss:2.0692576348195346\n",
      "train loss:2.078309071700958\n",
      "train loss:2.001067048436677\n",
      "train loss:1.9120166626031965\n",
      "train loss:1.983003713000328\n",
      "train loss:1.7461249182420915\n",
      "train loss:1.630739660656441\n",
      "train loss:1.6429634076810877\n",
      "train loss:1.5965574808933727\n",
      "train loss:1.5690923974589233\n",
      "train loss:1.4671596312295776\n",
      "train loss:1.296497814324545\n",
      "train loss:1.1654405255037172\n",
      "train loss:1.2009044120087558\n",
      "train loss:1.0858464424579939\n",
      "train loss:1.0260094331393008\n",
      "train loss:0.9789876575134757\n",
      "train loss:0.9400266885508417\n",
      "train loss:0.9119501240415686\n",
      "train loss:0.839389719067243\n",
      "train loss:0.8040769113712907\n",
      "train loss:0.7451680065252804\n",
      "train loss:0.7300514652163079\n",
      "train loss:0.909711204731679\n",
      "train loss:0.5471475708331418\n",
      "train loss:0.7193672595778402\n",
      "train loss:0.7819538734345735\n",
      "train loss:0.705292035838732\n",
      "train loss:0.633668279723581\n",
      "train loss:0.7889732678996182\n",
      "train loss:0.5563271058928363\n",
      "train loss:0.6873050006596327\n",
      "train loss:0.6207449780730095\n",
      "train loss:0.5084106413057788\n",
      "train loss:0.5325581974066814\n",
      "train loss:0.47741386092313803\n",
      "train loss:0.6283346721193541\n",
      "train loss:0.7475721463721858\n",
      "train loss:0.6610363633914066\n",
      "train loss:0.6946683386317457\n",
      "train loss:0.6018442004404199\n",
      "train loss:0.5884523794830488\n",
      "train loss:0.5608180880437456\n",
      "train loss:0.4978426931998801\n",
      "train loss:0.40350428186316334\n",
      "train loss:0.5287044229941565\n",
      "train loss:0.43580941143161894\n",
      "train loss:0.42179865076867545\n",
      "train loss:0.5299555758398418\n",
      "train loss:0.42844498515380636\n",
      "train loss:0.6271909636459846\n",
      "train loss:0.3201701726889468\n",
      "train loss:0.5506848118642487\n",
      "train loss:0.5406103393245308\n",
      "train loss:0.35766592440979006\n",
      "train loss:0.5017105059545452\n",
      "train loss:0.5499758792357031\n",
      "train loss:0.5408160898834243\n",
      "train loss:0.5003963398562078\n",
      "train loss:0.42078399772067704\n",
      "train loss:0.487766151829087\n",
      "train loss:0.508224044429953\n",
      "train loss:0.45092489562742855\n",
      "train loss:0.527789565890701\n",
      "train loss:0.4130322591808141\n",
      "train loss:0.4288911929695747\n",
      "train loss:0.43552138954322595\n",
      "train loss:0.5017725788733852\n",
      "train loss:0.5477336606993843\n",
      "train loss:0.33995404106362104\n",
      "train loss:0.3752198341255737\n",
      "train loss:0.6304839851048899\n",
      "train loss:0.46693532512595437\n",
      "train loss:0.47946646832983675\n",
      "train loss:0.3533074825686496\n",
      "train loss:0.4103038225069614\n",
      "train loss:0.3320293988244113\n",
      "train loss:0.4448118620220645\n",
      "train loss:0.273261355185664\n",
      "train loss:0.3483120644044456\n",
      "train loss:0.5538542360281306\n",
      "train loss:0.35279901481085657\n",
      "train loss:0.37393619224837854\n",
      "train loss:0.4691915010392638\n",
      "train loss:0.4084593593835546\n",
      "train loss:0.4281730810819233\n",
      "train loss:0.3565322453210669\n",
      "train loss:0.6627946129866402\n",
      "train loss:0.28099471186134894\n",
      "train loss:0.2825724630696725\n",
      "train loss:0.305194957837786\n",
      "train loss:0.5068034804781684\n",
      "train loss:0.6121000663097781\n",
      "train loss:0.4624960477393828\n",
      "train loss:0.30258188047605206\n",
      "train loss:0.41216316056904667\n",
      "train loss:0.3832384514689236\n",
      "train loss:0.3429314031064247\n",
      "train loss:0.4132968824254955\n",
      "train loss:0.3594932614045284\n",
      "train loss:0.32245906436601646\n",
      "train loss:0.334044149779202\n",
      "train loss:0.29151255835485773\n",
      "train loss:0.28671277062608513\n",
      "train loss:0.4725597070238286\n",
      "train loss:0.4785978087078822\n",
      "train loss:0.735834189621943\n",
      "train loss:0.39410241126935863\n",
      "train loss:0.4956474988330825\n",
      "train loss:0.48497771316327176\n",
      "train loss:0.32917790651719164\n",
      "train loss:0.20731487088503242\n",
      "train loss:0.4977077269683422\n",
      "train loss:0.330475230938893\n",
      "train loss:0.3291791505930029\n",
      "train loss:0.42261913146268654\n",
      "train loss:0.2588138006740327\n",
      "train loss:0.3752119723270297\n",
      "train loss:0.40159982545975526\n",
      "train loss:0.42551197719763073\n",
      "train loss:0.36902172204778183\n",
      "train loss:0.4784876148493575\n",
      "train loss:0.2649579146025427\n",
      "train loss:0.43203464122177837\n",
      "train loss:0.3340609669938901\n",
      "train loss:0.24247922857137\n",
      "train loss:0.3954239155969453\n",
      "train loss:0.30912025449639663\n",
      "train loss:0.29779940019202805\n",
      "train loss:0.30624901795457193\n",
      "train loss:0.3593665555210402\n",
      "train loss:0.3153214479276725\n",
      "train loss:0.42690255008540423\n",
      "train loss:0.23917276858069111\n",
      "train loss:0.3387094559751084\n",
      "train loss:0.3192572888408865\n",
      "train loss:0.23434804134024462\n",
      "train loss:0.27554154721919666\n",
      "train loss:0.45456017006249894\n",
      "train loss:0.21346361894148802\n",
      "train loss:0.30051568602551\n",
      "train loss:0.24715139314273382\n",
      "train loss:0.25244366289791165\n",
      "train loss:0.46203058759775567\n",
      "train loss:0.32593427099454786\n",
      "train loss:0.4243395160709542\n",
      "train loss:0.2807101580907312\n",
      "train loss:0.2567161427329526\n",
      "train loss:0.22582784823811689\n",
      "train loss:0.28066455537406315\n",
      "train loss:0.3454456251003986\n",
      "train loss:0.49345751935424054\n",
      "train loss:0.3217997109552417\n",
      "train loss:0.3574459822883254\n",
      "train loss:0.36396234425841223\n",
      "train loss:0.3347195847309123\n",
      "train loss:0.21318660525229824\n",
      "train loss:0.42393292294642654\n",
      "train loss:0.36545990315089555\n",
      "train loss:0.32735914791667786\n",
      "train loss:0.39646249047024645\n",
      "train loss:0.36767815728350756\n",
      "train loss:0.365248568155545\n",
      "train loss:0.32611769244090694\n",
      "train loss:0.24783624865730136\n",
      "train loss:0.26838905306394684\n",
      "train loss:0.256205119382409\n",
      "train loss:0.17839674392667462\n",
      "train loss:0.3934530154726279\n",
      "train loss:0.27368764449751876\n",
      "train loss:0.2999508154501274\n",
      "train loss:0.2878604008535552\n",
      "train loss:0.22424361896303477\n",
      "train loss:0.22255279055415994\n",
      "train loss:0.29928976381840755\n",
      "train loss:0.2543323549849648\n",
      "train loss:0.2301382330383059\n",
      "train loss:0.1850284913752309\n",
      "train loss:0.15460989607309003\n",
      "train loss:0.33895659191169014\n",
      "train loss:0.3503883490654246\n",
      "train loss:0.3634759239693354\n",
      "train loss:0.1725346984085188\n",
      "train loss:0.2189482308575778\n",
      "train loss:0.23007852315115015\n",
      "train loss:0.21051313221319184\n",
      "train loss:0.16066978121488412\n",
      "train loss:0.2631253565900301\n",
      "train loss:0.1568569279092097\n",
      "train loss:0.5099842383786745\n",
      "train loss:0.3451492321705309\n",
      "train loss:0.3414453169992819\n",
      "train loss:0.20890179891686908\n",
      "train loss:0.3299047633924652\n",
      "train loss:0.2584404702550591\n",
      "train loss:0.32407323785319264\n",
      "train loss:0.19773373323671556\n",
      "train loss:0.2616004324623148\n",
      "train loss:0.37143677884790677\n",
      "train loss:0.22503155872289982\n",
      "train loss:0.3209366384231916\n",
      "train loss:0.34583866746353614\n",
      "train loss:0.2824106233704434\n",
      "train loss:0.29495343791608714\n",
      "train loss:0.45836074727172454\n",
      "train loss:0.36111913709749965\n",
      "train loss:0.20795019126298475\n",
      "train loss:0.3125316563497152\n",
      "train loss:0.2661745352650241\n",
      "train loss:0.25229271185761076\n",
      "train loss:0.20264584849925854\n",
      "train loss:0.2537915463436078\n",
      "train loss:0.19982360895306578\n",
      "train loss:0.4164277453998207\n",
      "train loss:0.3429431268269604\n",
      "train loss:0.22640961367237494\n",
      "train loss:0.3589215563186737\n",
      "train loss:0.19959242334041224\n",
      "train loss:0.24364584274592502\n",
      "train loss:0.5300925621829351\n",
      "train loss:0.27201552121005806\n",
      "train loss:0.31203310925412986\n",
      "train loss:0.33543815094816204\n",
      "train loss:0.1235892619933589\n",
      "train loss:0.27873433058971914\n",
      "train loss:0.34641393539385623\n",
      "train loss:0.33315614381090486\n",
      "train loss:0.18605155293492262\n",
      "train loss:0.43940952126139315\n",
      "train loss:0.32405842758380204\n",
      "train loss:0.3455525219229307\n",
      "train loss:0.1156716547898337\n",
      "train loss:0.2542783646160178\n",
      "train loss:0.3251407873321059\n",
      "train loss:0.13392799646865516\n",
      "train loss:0.2500214186065178\n",
      "train loss:0.15579923193628734\n",
      "train loss:0.3553733109087037\n",
      "train loss:0.3236300335684227\n",
      "train loss:0.11854419281607109\n",
      "train loss:0.20820452315033325\n",
      "train loss:0.22406419179986364\n",
      "train loss:0.1952031719057799\n",
      "train loss:0.2582788100148649\n",
      "train loss:0.15819643334951894\n",
      "train loss:0.2666359211130747\n",
      "train loss:0.25039548049144633\n",
      "train loss:0.176592724234596\n",
      "train loss:0.21802887126577075\n",
      "train loss:0.3733114854310004\n",
      "train loss:0.2771165438224383\n",
      "train loss:0.3230806538708522\n",
      "train loss:0.3026811339595856\n",
      "train loss:0.19562357368963543\n",
      "train loss:0.25568567297315925\n",
      "train loss:0.3385873798411238\n",
      "train loss:0.21002520401846675\n",
      "train loss:0.15894192630684498\n",
      "train loss:0.2364709559035139\n",
      "train loss:0.22852366507000244\n",
      "train loss:0.1985729956903334\n",
      "train loss:0.2940935675377538\n",
      "train loss:0.19703392637255576\n",
      "train loss:0.28456570095558503\n",
      "train loss:0.287573178332281\n",
      "train loss:0.3293631727983747\n",
      "train loss:0.3248479830865416\n",
      "train loss:0.29758278346809613\n",
      "train loss:0.2505309179460533\n",
      "train loss:0.32199512936977975\n",
      "train loss:0.3589939227190012\n",
      "train loss:0.15867158672976317\n",
      "train loss:0.22071104944293993\n",
      "train loss:0.2382312725558703\n",
      "train loss:0.19816834857456134\n",
      "train loss:0.19958356939581892\n",
      "train loss:0.22910538983951734\n",
      "train loss:0.20146023910622302\n",
      "train loss:0.21277695604406563\n",
      "train loss:0.25540295849757955\n",
      "train loss:0.2016873731465666\n",
      "train loss:0.1853423222153776\n",
      "train loss:0.1013596230221964\n",
      "train loss:0.1626079927306964\n",
      "train loss:0.22894761578977899\n",
      "train loss:0.21977605153010554\n",
      "train loss:0.29456525823751734\n",
      "train loss:0.265180891634108\n",
      "train loss:0.1808072665464568\n",
      "train loss:0.2371769027900993\n",
      "train loss:0.2550521562641091\n",
      "train loss:0.17690084614472398\n",
      "train loss:0.17803500827197297\n",
      "train loss:0.17595401167323238\n",
      "train loss:0.23459228882582062\n",
      "train loss:0.21658986778109773\n",
      "train loss:0.1991842121991256\n",
      "train loss:0.16717906178932584\n",
      "train loss:0.22805080495152563\n",
      "train loss:0.31481055943837355\n",
      "train loss:0.16186486424245733\n",
      "train loss:0.23373901994130775\n",
      "train loss:0.20797041011527778\n",
      "train loss:0.3321352127236068\n",
      "train loss:0.1198759315957403\n",
      "train loss:0.12795716667554013\n",
      "train loss:0.22585143006949604\n",
      "train loss:0.25281357500896756\n",
      "train loss:0.3771602208893324\n",
      "train loss:0.36494556243101606\n",
      "train loss:0.2702344750878832\n",
      "train loss:0.2764802963434022\n",
      "train loss:0.14214901339256478\n",
      "train loss:0.3363751490145212\n",
      "train loss:0.23022586521368674\n",
      "train loss:0.3885251259581684\n",
      "train loss:0.34233426026586344\n",
      "train loss:0.15298662893649162\n",
      "train loss:0.19300746391947865\n",
      "train loss:0.15438595482764153\n",
      "train loss:0.12265257664199751\n",
      "train loss:0.1381459089322264\n",
      "train loss:0.35649893659790516\n",
      "train loss:0.12875436118490463\n",
      "train loss:0.19568688411072968\n",
      "train loss:0.14993083121161835\n",
      "train loss:0.17720595776590295\n",
      "train loss:0.29882143954199536\n",
      "train loss:0.19150953870175433\n",
      "train loss:0.2963332790422829\n",
      "train loss:0.2109128571807252\n",
      "train loss:0.24669918643687513\n",
      "train loss:0.24339181767543494\n",
      "train loss:0.2499771146420727\n",
      "train loss:0.16433745099014896\n",
      "train loss:0.2509415119156298\n",
      "train loss:0.1413318187936049\n",
      "train loss:0.36198685644966133\n",
      "train loss:0.12334424036867418\n",
      "train loss:0.1957209660509265\n",
      "train loss:0.2254260234502006\n",
      "train loss:0.20786317502494853\n",
      "train loss:0.12196968506686481\n",
      "train loss:0.29082229440224056\n",
      "train loss:0.11824152619856379\n",
      "train loss:0.21623718262003838\n",
      "train loss:0.16182789225004665\n",
      "train loss:0.21144707874132876\n",
      "train loss:0.14101601411172612\n",
      "train loss:0.24621669960313564\n",
      "train loss:0.27793368139954444\n",
      "train loss:0.29714584420938195\n",
      "train loss:0.1956812965273765\n",
      "train loss:0.2219271114080599\n",
      "train loss:0.17444628783671978\n",
      "train loss:0.1892942185841602\n",
      "train loss:0.2121137968973802\n",
      "train loss:0.14152314379889389\n",
      "train loss:0.16672241137862154\n",
      "train loss:0.20883304073639447\n",
      "train loss:0.1016250603644631\n",
      "train loss:0.2611186840177879\n",
      "train loss:0.23572170466388073\n",
      "train loss:0.16336910652553654\n",
      "train loss:0.23277095905429646\n",
      "train loss:0.2891282659787964\n",
      "train loss:0.28227894953581956\n",
      "train loss:0.257239698724751\n",
      "train loss:0.11011893897527285\n",
      "train loss:0.2766045140156384\n",
      "train loss:0.10701135792129034\n",
      "train loss:0.17321061013792416\n",
      "train loss:0.10279622983174623\n",
      "train loss:0.16265800120772755\n",
      "train loss:0.2728958974826665\n",
      "train loss:0.12061035093569344\n",
      "train loss:0.18706001429844718\n",
      "train loss:0.10053205160510542\n",
      "train loss:0.1849916049514567\n",
      "train loss:0.15073624409231492\n",
      "train loss:0.15177620815417397\n",
      "train loss:0.09901326966517356\n",
      "train loss:0.11701134255977177\n",
      "train loss:0.08400918264109439\n",
      "train loss:0.20458901177880837\n",
      "train loss:0.20706175470262256\n",
      "train loss:0.1399847068157014\n",
      "train loss:0.317822018684518\n",
      "train loss:0.0651559981167152\n",
      "train loss:0.18358764730444052\n",
      "train loss:0.0863489183805821\n",
      "train loss:0.11154694665470184\n",
      "train loss:0.216587464724615\n",
      "train loss:0.11445292764623297\n",
      "train loss:0.1434369830108508\n",
      "train loss:0.14047262898708898\n",
      "train loss:0.3176518794953652\n",
      "train loss:0.2434834318829521\n",
      "train loss:0.17464233978665397\n",
      "train loss:0.23145997324890388\n",
      "train loss:0.10666831723417597\n",
      "train loss:0.3380139081182052\n",
      "train loss:0.3456418702009796\n",
      "train loss:0.15924126710494815\n",
      "train loss:0.2244683642301249\n",
      "train loss:0.23621140548191127\n",
      "train loss:0.2323812357354019\n",
      "train loss:0.19399577220723274\n",
      "train loss:0.14497521264585214\n",
      "train loss:0.17547262800380337\n",
      "train loss:0.12744016967237648\n",
      "train loss:0.1417492006858552\n",
      "train loss:0.1766979611529315\n",
      "train loss:0.1332312236656886\n",
      "train loss:0.13470923168783547\n",
      "train loss:0.1815648009115071\n",
      "train loss:0.2777168827262207\n",
      "train loss:0.16178262782252995\n",
      "train loss:0.14380585921841857\n",
      "train loss:0.2637329041159477\n",
      "train loss:0.24516928270793642\n",
      "train loss:0.11378455745985809\n",
      "train loss:0.15649966547292346\n",
      "train loss:0.1345447974400813\n",
      "train loss:0.339413417280224\n",
      "train loss:0.2330875190899511\n",
      "train loss:0.14354486738997246\n",
      "train loss:0.07252030419083433\n",
      "train loss:0.1604240270523104\n",
      "train loss:0.1347468696799158\n",
      "train loss:0.13516361322296067\n",
      "train loss:0.11796076526202097\n",
      "train loss:0.07670780989178821\n",
      "train loss:0.24223074806490744\n",
      "train loss:0.1194617541302788\n",
      "train loss:0.12465428544790352\n",
      "train loss:0.15588185441398522\n",
      "train loss:0.1264459162338693\n",
      "train loss:0.08341019589149376\n",
      "train loss:0.09566166585796243\n",
      "train loss:0.22265628637034265\n",
      "train loss:0.14280125994466475\n",
      "train loss:0.19643720488512442\n",
      "train loss:0.21536794301536866\n",
      "train loss:0.2192120200449658\n",
      "train loss:0.10962533057923278\n",
      "train loss:0.24153085295698307\n",
      "train loss:0.08718785075521357\n",
      "train loss:0.1367950604695943\n",
      "train loss:0.13615255365749315\n",
      "train loss:0.15485433196459156\n",
      "train loss:0.13537653534168004\n",
      "train loss:0.08393074885742356\n",
      "train loss:0.16812867823702715\n",
      "train loss:0.17703569956707466\n",
      "train loss:0.15024080936894108\n",
      "train loss:0.1770976906261129\n",
      "train loss:0.08581628054257637\n",
      "train loss:0.11902705825663416\n",
      "train loss:0.17122076912195328\n",
      "train loss:0.29860635476392533\n",
      "train loss:0.1911264205034752\n",
      "train loss:0.17884921400580545\n",
      "train loss:0.1831476036713983\n",
      "train loss:0.11092380998256197\n",
      "train loss:0.17391783643675388\n",
      "train loss:0.2156478586419124\n",
      "train loss:0.15623559690963262\n",
      "train loss:0.10652246029680963\n",
      "train loss:0.21843647477082115\n",
      "train loss:0.1427472717795801\n",
      "train loss:0.1455636290671657\n",
      "train loss:0.0919291074764365\n",
      "train loss:0.14442115717400156\n",
      "train loss:0.14474718956383076\n",
      "train loss:0.22380762059458367\n",
      "train loss:0.19780744830680486\n",
      "train loss:0.2691981634602547\n",
      "train loss:0.19974777668025934\n",
      "train loss:0.17064360199106116\n",
      "train loss:0.15858079320526058\n",
      "train loss:0.20113408706787347\n",
      "train loss:0.25154859608024976\n",
      "train loss:0.12590777920575838\n",
      "train loss:0.2072165605096621\n",
      "train loss:0.14519909455046132\n",
      "train loss:0.10046603180704478\n",
      "train loss:0.15709076204762107\n",
      "train loss:0.09600228755328905\n",
      "train loss:0.08155079804093529\n",
      "train loss:0.09915430105253115\n",
      "train loss:0.21485051356191046\n",
      "train loss:0.17340059833757393\n",
      "train loss:0.11384118934210878\n",
      "train loss:0.060780335037777265\n",
      "train loss:0.20758890704634073\n",
      "train loss:0.05915204770679277\n",
      "train loss:0.12094596798248287\n",
      "train loss:0.1978088867129207\n",
      "train loss:0.09136909681300455\n",
      "train loss:0.11006190205501594\n",
      "train loss:0.2522015414154011\n",
      "train loss:0.07919180809385643\n",
      "train loss:0.07527861222867274\n",
      "train loss:0.17647117083139438\n",
      "train loss:0.14494641858262097\n",
      "train loss:0.1690716320480014\n",
      "train loss:0.15276237992446007\n",
      "train loss:0.2340532673060034\n",
      "train loss:0.2235986813428788\n",
      "train loss:0.15031273621462662\n",
      "train loss:0.17240425172327925\n",
      "train loss:0.19415559592043574\n",
      "train loss:0.12517242068955903\n",
      "train loss:0.1689457063693511\n",
      "train loss:0.0778389572968343\n",
      "train loss:0.07199140747854195\n",
      "train loss:0.18632854599029958\n",
      "train loss:0.04926660799247125\n",
      "train loss:0.17218797154853643\n",
      "train loss:0.1244868058486386\n",
      "train loss:0.20598197047102673\n",
      "train loss:0.19501834603970025\n",
      "train loss:0.18086551112269358\n",
      "train loss:0.22944941365291016\n",
      "train loss:0.167199232827675\n",
      "train loss:0.13143412508691021\n",
      "train loss:0.07152245643800226\n",
      "train loss:0.0990855590599022\n",
      "train loss:0.2209125999796855\n",
      "train loss:0.14477924691731456\n",
      "train loss:0.16196372251571575\n",
      "train loss:0.11821717893420385\n",
      "train loss:0.12292328506777059\n",
      "train loss:0.13678124390988033\n",
      "train loss:0.0660666013726244\n",
      "train loss:0.07537933494808916\n",
      "train loss:0.0599990390939394\n",
      "train loss:0.10521051446567777\n",
      "train loss:0.13281921276721173\n",
      "train loss:0.09170762439043605\n",
      "train loss:0.10227832148111204\n",
      "train loss:0.1397359923275406\n",
      "train loss:0.09379134854920108\n",
      "train loss:0.1510482299891102\n",
      "train loss:0.11812760383770328\n",
      "train loss:0.15001516138934307\n",
      "train loss:0.12947630041145486\n",
      "train loss:0.10688063662911418\n",
      "train loss:0.13301685784090295\n",
      "train loss:0.0890043924502459\n",
      "train loss:0.1873611984607154\n",
      "train loss:0.2156720349847558\n",
      "train loss:0.09682512852051774\n",
      "train loss:0.13969374347077304\n",
      "train loss:0.07066256692856909\n",
      "train loss:0.10807023599636555\n",
      "train loss:0.14152837102373986\n",
      "train loss:0.08549976137178569\n",
      "train loss:0.09447374065982019\n",
      "train loss:0.12847482976497415\n",
      "train loss:0.0798764803431242\n",
      "train loss:0.08792849789055257\n",
      "train loss:0.07802261623139983\n",
      "train loss:0.15970713924661478\n",
      "train loss:0.2779032332669165\n",
      "train loss:0.0910008645423321\n",
      "train loss:0.16433794308197616\n",
      "train loss:0.21023296448234446\n",
      "train loss:0.12186030573847907\n",
      "train loss:0.06374043040388874\n",
      "train loss:0.09890968346284917\n",
      "train loss:0.07434667839478638\n",
      "train loss:0.3004152771410803\n",
      "train loss:0.11757197565572194\n",
      "train loss:0.0673284720884117\n",
      "train loss:0.19595333923401662\n",
      "train loss:0.07579145222215311\n",
      "train loss:0.026826223912398327\n",
      "train loss:0.20462296003020433\n",
      "train loss:0.13623821758012553\n",
      "train loss:0.18690625504371053\n",
      "train loss:0.20664612653356443\n",
      "train loss:0.06457841017671498\n",
      "train loss:0.06698216816364799\n",
      "train loss:0.0651643204427302\n",
      "train loss:0.12698087460094717\n",
      "train loss:0.0949801242486168\n",
      "train loss:0.11192637243273953\n",
      "train loss:0.18082482172829775\n",
      "=== epoch:2, train acc:0.952, test acc:0.95 ===\n",
      "train loss:0.132894670600321\n",
      "train loss:0.19814900919679646\n",
      "train loss:0.06500341126008227\n",
      "train loss:0.11279661594629346\n",
      "train loss:0.11256505376625521\n",
      "train loss:0.2005681403304785\n",
      "train loss:0.07300325882824708\n",
      "train loss:0.05026990304130779\n",
      "train loss:0.12863009906952969\n",
      "train loss:0.11749390774545214\n",
      "train loss:0.05570033456087563\n",
      "train loss:0.037476585625383294\n",
      "train loss:0.07215185460406594\n",
      "train loss:0.07418929165290244\n",
      "train loss:0.08222973671633295\n",
      "train loss:0.07634370873270682\n",
      "train loss:0.03001037256432342\n",
      "train loss:0.24373931619327954\n",
      "train loss:0.043024544024296105\n",
      "train loss:0.20737559541739267\n",
      "train loss:0.09466820968522965\n",
      "train loss:0.15286786481034675\n",
      "train loss:0.048432447320457574\n",
      "train loss:0.15060122092115022\n",
      "train loss:0.163566395312485\n",
      "train loss:0.10021822268330852\n",
      "train loss:0.19131546144105205\n",
      "train loss:0.21433855142159697\n",
      "train loss:0.14359031515810575\n",
      "train loss:0.08598047381777515\n",
      "train loss:0.11002354290528624\n",
      "train loss:0.16036999112198663\n",
      "train loss:0.12047776713198662\n",
      "train loss:0.040786631315713244\n",
      "train loss:0.11599325692758528\n",
      "train loss:0.0810801174040311\n",
      "train loss:0.05549394757321353\n",
      "train loss:0.15295913894121338\n",
      "train loss:0.17578451782860147\n",
      "train loss:0.09738073699014031\n",
      "train loss:0.1278756353892145\n",
      "train loss:0.047183228014299416\n",
      "train loss:0.06880078869452805\n",
      "train loss:0.09635065009529546\n",
      "train loss:0.13586650929959476\n",
      "train loss:0.07686906623448643\n",
      "train loss:0.113700917562833\n",
      "train loss:0.12859696088116446\n",
      "train loss:0.13075925838904615\n",
      "train loss:0.10655105876699214\n",
      "train loss:0.07336487266179309\n",
      "train loss:0.10183089055540848\n",
      "train loss:0.08230876242349697\n",
      "train loss:0.07658878470825287\n",
      "train loss:0.25083839681743497\n",
      "train loss:0.12427334053658656\n",
      "train loss:0.05546177317983835\n",
      "train loss:0.19637534172561769\n",
      "train loss:0.09641937510636815\n",
      "train loss:0.1939615685807192\n",
      "train loss:0.09189214034381393\n",
      "train loss:0.047853166155630046\n",
      "train loss:0.08843439708912867\n",
      "train loss:0.1246210505051593\n",
      "train loss:0.09001768604462695\n",
      "train loss:0.15930051045578766\n",
      "train loss:0.22483084643752105\n",
      "train loss:0.040479207846835334\n",
      "train loss:0.14527582078988474\n",
      "train loss:0.13242812348010527\n",
      "train loss:0.09007294162470414\n",
      "train loss:0.05761364771191815\n",
      "train loss:0.1754456077442187\n",
      "train loss:0.06065161175927973\n",
      "train loss:0.07806187991302574\n",
      "train loss:0.10093575388380824\n",
      "train loss:0.09907200598961802\n",
      "train loss:0.08040053426792436\n",
      "train loss:0.18721500355677484\n",
      "train loss:0.08905715699171282\n",
      "train loss:0.13000712519638538\n",
      "train loss:0.05535058177171118\n",
      "train loss:0.10192987265261744\n",
      "train loss:0.057291270282624\n",
      "train loss:0.07938993831223388\n",
      "train loss:0.09504361064257875\n",
      "train loss:0.11113427565877956\n",
      "train loss:0.0522327797197845\n",
      "train loss:0.0678263220316074\n",
      "train loss:0.11693123283659845\n",
      "train loss:0.08973651504235913\n",
      "train loss:0.12926611456534812\n",
      "train loss:0.0812379087911335\n",
      "train loss:0.07908401154909839\n",
      "train loss:0.14540667339370741\n",
      "train loss:0.12120401189043563\n",
      "train loss:0.1177512741806301\n",
      "train loss:0.15754297739268006\n",
      "train loss:0.14119840236208753\n",
      "train loss:0.05485808789342005\n",
      "train loss:0.1330204553795393\n",
      "train loss:0.08803341124445901\n",
      "train loss:0.12841458838044223\n",
      "train loss:0.028317976880410618\n",
      "train loss:0.09505353296194481\n",
      "train loss:0.06781386644430962\n",
      "train loss:0.07010195653139795\n",
      "train loss:0.1067072588889757\n",
      "train loss:0.15348460793532076\n",
      "train loss:0.14176527481356282\n",
      "train loss:0.04557655066309108\n",
      "train loss:0.07863648124808639\n",
      "train loss:0.12764457311168587\n",
      "train loss:0.23170274658383433\n",
      "train loss:0.056202820666147325\n",
      "train loss:0.1674589681417773\n",
      "train loss:0.0631470997576499\n",
      "train loss:0.04375928211701084\n",
      "train loss:0.18804417591608075\n",
      "train loss:0.04546356354058598\n",
      "train loss:0.061930644255782374\n",
      "train loss:0.07585839026713753\n",
      "train loss:0.16710514407850913\n",
      "train loss:0.08909022491632135\n",
      "train loss:0.13389522383776256\n",
      "train loss:0.1047790070522684\n",
      "train loss:0.07182461902174796\n",
      "train loss:0.10936607205390138\n",
      "train loss:0.04755542992864086\n",
      "train loss:0.0971499513067709\n",
      "train loss:0.13728113193695934\n",
      "train loss:0.07107793131358761\n",
      "train loss:0.10819378251064203\n",
      "train loss:0.08562720689487709\n",
      "train loss:0.1129974135687141\n",
      "train loss:0.07990463590441793\n",
      "train loss:0.07121911325832149\n",
      "train loss:0.2041297061655964\n",
      "train loss:0.2431545091466042\n",
      "train loss:0.06368208127624841\n",
      "train loss:0.06216301187746485\n",
      "train loss:0.04515203146004334\n",
      "train loss:0.11116294112852537\n",
      "train loss:0.12072697182852696\n",
      "train loss:0.05127352088814421\n",
      "train loss:0.17093399821083038\n",
      "train loss:0.07759471177595345\n",
      "train loss:0.1956043228909483\n",
      "train loss:0.04650392962965673\n",
      "train loss:0.13754991708844488\n",
      "train loss:0.07663417913843101\n",
      "train loss:0.188270593800804\n",
      "train loss:0.04181079383416386\n",
      "train loss:0.2071830687781486\n",
      "train loss:0.05312602724714454\n",
      "train loss:0.11415195109590655\n",
      "train loss:0.09451497588711101\n",
      "train loss:0.1423513527682535\n",
      "train loss:0.06247890710597738\n",
      "train loss:0.06953997954120954\n",
      "train loss:0.13442920786895096\n",
      "train loss:0.11532399801067228\n",
      "train loss:0.0835182582635254\n",
      "train loss:0.11668495267711426\n",
      "train loss:0.06286455386208373\n",
      "train loss:0.1571787547306007\n",
      "train loss:0.12010167348378976\n",
      "train loss:0.12868816820551693\n",
      "train loss:0.11081050783775873\n",
      "train loss:0.1330160506852823\n",
      "train loss:0.06658591700349657\n",
      "train loss:0.10954189008913456\n",
      "train loss:0.08043498820043221\n",
      "train loss:0.0630447640967246\n",
      "train loss:0.1148209474887262\n",
      "train loss:0.21664704392387824\n",
      "train loss:0.05012520733114669\n",
      "train loss:0.09387679223949631\n",
      "train loss:0.08028996535479198\n",
      "train loss:0.06710702299784575\n",
      "train loss:0.08410111938458233\n",
      "train loss:0.08475837089459941\n",
      "train loss:0.12057624765536787\n",
      "train loss:0.05478644847337617\n",
      "train loss:0.08621002536987168\n",
      "train loss:0.06349062787457027\n",
      "train loss:0.11736312773116707\n",
      "train loss:0.19597435723718018\n",
      "train loss:0.06617621075629866\n",
      "train loss:0.13229304051591292\n",
      "train loss:0.0568529056718216\n",
      "train loss:0.07430595031736362\n",
      "train loss:0.08205637932042269\n",
      "train loss:0.08017195632837655\n",
      "train loss:0.11634093820204174\n",
      "train loss:0.12556548668472853\n",
      "train loss:0.13739792118679137\n",
      "train loss:0.046715347182593604\n",
      "train loss:0.0732693201542955\n",
      "train loss:0.1391155759922804\n",
      "train loss:0.0809639147368521\n",
      "train loss:0.05008000093723953\n",
      "train loss:0.10021716408324707\n",
      "train loss:0.12414226439605827\n",
      "train loss:0.07679614784447075\n",
      "train loss:0.10587738324705001\n",
      "train loss:0.07865839312748682\n",
      "train loss:0.09896701465702802\n",
      "train loss:0.1467388941146575\n",
      "train loss:0.06803542038680678\n",
      "train loss:0.06383030214844246\n",
      "train loss:0.034036580835850996\n",
      "train loss:0.07142722908256183\n",
      "train loss:0.1972417285796959\n",
      "train loss:0.0732011773212455\n",
      "train loss:0.07465548197462708\n",
      "train loss:0.03734213030447552\n",
      "train loss:0.1032677801752835\n",
      "train loss:0.16829785579695042\n",
      "train loss:0.08285594418323289\n",
      "train loss:0.026062028484617597\n",
      "train loss:0.15593064678581178\n",
      "train loss:0.06591416567466739\n",
      "train loss:0.10626210162678566\n",
      "train loss:0.08691959311936637\n",
      "train loss:0.0897927992000085\n",
      "train loss:0.1443950334269209\n",
      "train loss:0.03345717199907476\n",
      "train loss:0.09622440350912465\n",
      "train loss:0.16631112414603533\n",
      "train loss:0.20357875682577864\n",
      "train loss:0.21048864774552778\n",
      "train loss:0.030799302937402726\n",
      "train loss:0.10607331702181347\n",
      "train loss:0.10656155976753233\n",
      "train loss:0.08829012796526094\n",
      "train loss:0.07278834013289384\n",
      "train loss:0.09957083335911204\n",
      "train loss:0.18118070799065442\n",
      "train loss:0.13324387968609602\n",
      "train loss:0.0732346380533338\n",
      "train loss:0.06624859725411689\n",
      "train loss:0.10905877482261632\n",
      "train loss:0.04762970043484094\n",
      "train loss:0.17076337124794122\n",
      "train loss:0.07066006054237643\n",
      "train loss:0.11004185621357236\n",
      "train loss:0.02359664856386348\n",
      "train loss:0.1778000483686927\n",
      "train loss:0.09494653972896602\n",
      "train loss:0.1215177563738289\n",
      "train loss:0.09953653230542121\n",
      "train loss:0.06909768984874459\n",
      "train loss:0.12107738603244657\n",
      "train loss:0.23116407785260318\n",
      "train loss:0.060865880935660345\n",
      "train loss:0.04203860246950228\n",
      "train loss:0.17342551063121373\n",
      "train loss:0.05137009164996405\n",
      "train loss:0.10096509839343382\n",
      "train loss:0.06517684222207391\n",
      "train loss:0.11880922162636848\n",
      "train loss:0.10162519804734585\n",
      "train loss:0.04511485895342643\n",
      "train loss:0.13649452805889062\n",
      "train loss:0.07744887295899887\n",
      "train loss:0.08229103383865963\n",
      "train loss:0.0973756608746522\n",
      "train loss:0.09981837834229483\n",
      "train loss:0.0841058044798505\n",
      "train loss:0.10032305727340177\n",
      "train loss:0.02492724524669017\n",
      "train loss:0.09135589353317199\n",
      "train loss:0.1226735032589339\n",
      "train loss:0.10296973623045828\n",
      "train loss:0.08115596195979721\n",
      "train loss:0.08488198933177625\n",
      "train loss:0.09963801078730594\n",
      "train loss:0.08844545559303625\n",
      "train loss:0.06474381627254712\n",
      "train loss:0.07780149695720501\n",
      "train loss:0.05104632831514711\n",
      "train loss:0.03637073290517745\n",
      "train loss:0.05453678818513127\n",
      "train loss:0.1290157944242533\n",
      "train loss:0.1605215783052882\n",
      "train loss:0.0752726981746433\n",
      "train loss:0.09906170884220883\n",
      "train loss:0.10517270963109743\n",
      "train loss:0.07300166829031579\n",
      "train loss:0.036836783915579846\n",
      "train loss:0.015319910736847213\n",
      "train loss:0.058923684232154445\n",
      "train loss:0.045041108663308994\n",
      "train loss:0.10218634363065644\n",
      "train loss:0.05748977359587729\n",
      "train loss:0.064838174703152\n",
      "train loss:0.08786054393966819\n",
      "train loss:0.04832500080711346\n",
      "train loss:0.1160855302399272\n",
      "train loss:0.054589635457403446\n",
      "train loss:0.08399313476658536\n",
      "train loss:0.06468025791276202\n",
      "train loss:0.145152512223138\n",
      "train loss:0.10093713153967354\n",
      "train loss:0.042775778807655554\n",
      "train loss:0.05688837616156837\n",
      "train loss:0.05421846275553599\n",
      "train loss:0.10029813172450265\n",
      "train loss:0.060254573589143635\n",
      "train loss:0.048218494892327525\n",
      "train loss:0.08118157087817521\n",
      "train loss:0.10042332438447352\n",
      "train loss:0.06337062903522589\n",
      "train loss:0.04808335914833028\n",
      "train loss:0.11443354712853271\n",
      "train loss:0.08417762464718313\n",
      "train loss:0.022612111577771397\n",
      "train loss:0.08371089572005888\n",
      "train loss:0.08369347748409808\n",
      "train loss:0.09028021118390182\n",
      "train loss:0.09231167723613089\n",
      "train loss:0.13198492572890877\n",
      "train loss:0.12572330123534045\n",
      "train loss:0.13802705708170776\n",
      "train loss:0.10360078628162242\n",
      "train loss:0.06133649959444827\n",
      "train loss:0.08535938724455294\n",
      "train loss:0.08527571252381516\n",
      "train loss:0.19337937133368632\n",
      "train loss:0.08646914287109801\n",
      "train loss:0.05531687588160078\n",
      "train loss:0.13607925702114262\n",
      "train loss:0.045494655486936326\n",
      "train loss:0.050787018776752804\n",
      "train loss:0.11225578951764913\n",
      "train loss:0.09506881895259392\n",
      "train loss:0.06967692458368724\n",
      "train loss:0.06279188876491053\n",
      "train loss:0.06301001973975666\n",
      "train loss:0.06953534965312375\n",
      "train loss:0.09053009437959812\n",
      "train loss:0.06427264078010314\n",
      "train loss:0.02164972502668719\n",
      "train loss:0.0501206862073387\n",
      "train loss:0.033033366998630076\n",
      "train loss:0.09554553594139537\n",
      "train loss:0.0888900238243251\n",
      "train loss:0.06855370969301927\n",
      "train loss:0.04277695524721424\n",
      "train loss:0.12027854151460307\n",
      "train loss:0.06445066694012597\n",
      "train loss:0.0771818004173894\n",
      "train loss:0.06369973843281083\n",
      "train loss:0.1168656904605667\n",
      "train loss:0.07156559709501342\n",
      "train loss:0.13404983171694868\n",
      "train loss:0.09184923023989476\n",
      "train loss:0.11800680173349729\n",
      "train loss:0.03501074602664811\n",
      "train loss:0.08341107768891931\n",
      "train loss:0.026905963277251314\n",
      "train loss:0.055240715566897476\n",
      "train loss:0.10667897773562976\n",
      "train loss:0.12242145419666514\n",
      "train loss:0.05299689850193547\n",
      "train loss:0.04987598465048784\n",
      "train loss:0.06064498322005108\n",
      "train loss:0.10268430925078455\n",
      "train loss:0.02762422578227099\n",
      "train loss:0.05615639206348654\n",
      "train loss:0.17813672343932507\n",
      "train loss:0.07068382450567122\n",
      "train loss:0.07404979897542598\n",
      "train loss:0.07329376402954786\n",
      "train loss:0.05320359947153138\n",
      "train loss:0.046172292610456314\n",
      "train loss:0.026593052774374567\n",
      "train loss:0.05236063730639723\n",
      "train loss:0.06567818595938738\n",
      "train loss:0.06070765900214922\n",
      "train loss:0.07724885125057926\n",
      "train loss:0.07017724774835572\n",
      "train loss:0.06179991627614081\n",
      "train loss:0.024029031953857825\n",
      "train loss:0.05476007982396833\n",
      "train loss:0.054313816621032664\n",
      "train loss:0.04938308441368575\n",
      "train loss:0.17242509543360499\n",
      "train loss:0.025340126417681342\n",
      "train loss:0.05245839298755445\n",
      "train loss:0.058323200048325835\n",
      "train loss:0.03514373613040367\n",
      "train loss:0.16756978596784858\n",
      "train loss:0.08733804519005525\n",
      "train loss:0.0549752847525176\n",
      "train loss:0.09322832421124501\n",
      "train loss:0.07222915263925257\n",
      "train loss:0.04783698143760755\n",
      "train loss:0.11075889557284206\n",
      "train loss:0.045135860870285954\n",
      "train loss:0.040544507054333205\n",
      "train loss:0.05095387438923247\n",
      "train loss:0.052447595285381336\n",
      "train loss:0.06475963102484339\n",
      "train loss:0.08146447111621886\n",
      "train loss:0.03787674001819894\n",
      "train loss:0.05517550831691375\n",
      "train loss:0.11639347787043841\n",
      "train loss:0.011390920582826435\n",
      "train loss:0.04352804781738259\n",
      "train loss:0.06006716877082254\n",
      "train loss:0.07988793026910686\n",
      "train loss:0.06836779831212683\n",
      "train loss:0.2825598091241929\n",
      "train loss:0.036208522492397345\n",
      "train loss:0.047567129126924367\n",
      "train loss:0.0780924694393385\n",
      "train loss:0.038165875005247304\n",
      "train loss:0.06656860866446114\n",
      "train loss:0.038352035506602516\n",
      "train loss:0.0853538945569269\n",
      "train loss:0.09184408508289145\n",
      "train loss:0.07325685154972646\n",
      "train loss:0.036886867319937316\n",
      "train loss:0.04947830186292922\n",
      "train loss:0.054826140940580394\n",
      "train loss:0.011399852804998915\n",
      "train loss:0.15266341285319518\n",
      "train loss:0.038314747155237416\n",
      "train loss:0.1045264791373463\n",
      "train loss:0.06482548641909061\n",
      "train loss:0.08212959207226213\n",
      "train loss:0.05511427776708851\n",
      "train loss:0.11133125046289574\n",
      "train loss:0.05175335469454333\n",
      "train loss:0.0783945269142539\n",
      "train loss:0.22981161183425866\n",
      "train loss:0.03994334253475411\n",
      "train loss:0.0656995183706968\n",
      "train loss:0.08482840343680526\n",
      "train loss:0.08882299313582566\n",
      "train loss:0.09668711258559558\n",
      "train loss:0.04950109783715759\n",
      "train loss:0.057418242292449004\n",
      "train loss:0.08049545114172911\n",
      "train loss:0.09014872623288823\n",
      "train loss:0.03367050338334641\n",
      "train loss:0.059709694125036517\n",
      "train loss:0.06943196580610399\n",
      "train loss:0.09845915649273212\n",
      "train loss:0.07453989042727392\n",
      "train loss:0.1251418085166964\n",
      "train loss:0.07466940528411546\n",
      "train loss:0.058625408432128955\n",
      "train loss:0.03555045344608402\n",
      "train loss:0.1465330718257974\n",
      "train loss:0.07460781794592675\n",
      "train loss:0.07924016650865463\n",
      "train loss:0.05311036835673032\n",
      "train loss:0.03865734730639375\n",
      "train loss:0.08816368146325011\n",
      "train loss:0.08646455959923474\n",
      "train loss:0.06425594736000352\n",
      "train loss:0.0638738987971835\n",
      "train loss:0.05781311802759888\n",
      "train loss:0.07181817144356922\n",
      "train loss:0.043294717736933136\n",
      "train loss:0.1432498480122265\n",
      "train loss:0.0855347467043253\n",
      "train loss:0.1400809384778137\n",
      "train loss:0.01574889524780635\n",
      "train loss:0.06272349532085907\n",
      "train loss:0.05888346539263849\n",
      "train loss:0.08354977348621435\n",
      "train loss:0.12850003242988964\n",
      "train loss:0.022522156392756332\n",
      "train loss:0.07201347521480997\n",
      "train loss:0.029697481246004555\n",
      "train loss:0.10464567546525454\n",
      "train loss:0.04570897346190259\n",
      "train loss:0.019374621581526405\n",
      "train loss:0.013207752788114955\n",
      "train loss:0.08831347638613035\n",
      "train loss:0.08894045346350225\n",
      "train loss:0.056334041530059685\n",
      "train loss:0.03478606693438383\n",
      "train loss:0.08467102605262557\n",
      "train loss:0.018101349498530758\n",
      "train loss:0.031505499224657646\n",
      "train loss:0.0981213684078891\n",
      "train loss:0.03437412158863305\n",
      "train loss:0.07254866081784848\n",
      "train loss:0.09858441687851213\n",
      "train loss:0.06591533034483753\n",
      "train loss:0.04251029164850731\n",
      "train loss:0.04669430992409896\n",
      "train loss:0.0751757037394417\n",
      "train loss:0.09083522996657939\n",
      "train loss:0.0927754978134487\n",
      "train loss:0.07543535808511467\n",
      "train loss:0.032959271611290135\n",
      "train loss:0.13940755616293665\n",
      "train loss:0.11234666428858979\n",
      "train loss:0.09442702309640037\n",
      "train loss:0.10009885463125333\n",
      "train loss:0.14973664514841306\n",
      "train loss:0.08713435371540525\n",
      "train loss:0.07924784019256367\n",
      "train loss:0.044894649667109335\n",
      "train loss:0.07169452457844194\n",
      "train loss:0.10537616232063605\n",
      "train loss:0.033524327462638934\n",
      "train loss:0.06872630361964678\n",
      "train loss:0.05625582944654993\n",
      "train loss:0.0983595441358919\n",
      "train loss:0.06656746832079959\n",
      "train loss:0.0921393360028289\n",
      "train loss:0.078541395934262\n",
      "train loss:0.05469244667953741\n",
      "train loss:0.0835000429698375\n",
      "train loss:0.03937015441728069\n",
      "train loss:0.04126808851646345\n",
      "train loss:0.037557476513787845\n",
      "train loss:0.021744797639732468\n",
      "train loss:0.03930095774732048\n",
      "train loss:0.06265579471092167\n",
      "train loss:0.0403154630509592\n",
      "train loss:0.1240106431721269\n",
      "train loss:0.16720587018668856\n",
      "train loss:0.031195846075139016\n",
      "train loss:0.017151931898278343\n",
      "train loss:0.05187867080305594\n",
      "train loss:0.0897930802792997\n",
      "train loss:0.06706889919726926\n",
      "train loss:0.13992830465101685\n",
      "train loss:0.05809819127357207\n",
      "train loss:0.04516765755727234\n",
      "train loss:0.045190204827534365\n",
      "train loss:0.05180324430920618\n",
      "train loss:0.07019846805623728\n",
      "train loss:0.10489665748493229\n",
      "train loss:0.07100481554488412\n",
      "train loss:0.04736580171347771\n",
      "train loss:0.06353388494493256\n",
      "train loss:0.06390437632523652\n",
      "train loss:0.03611008169341078\n",
      "train loss:0.0418557882697198\n",
      "train loss:0.07248081707264024\n",
      "train loss:0.09662974048020273\n",
      "train loss:0.03965577343495068\n",
      "train loss:0.1376343366158308\n",
      "train loss:0.07294434860304283\n",
      "train loss:0.042893316206222186\n",
      "train loss:0.09972836333246832\n",
      "train loss:0.029692416775301152\n",
      "train loss:0.0693015842350139\n",
      "train loss:0.1081745731842832\n",
      "train loss:0.059628351999417246\n",
      "train loss:0.09673318570565029\n",
      "train loss:0.08730767290370124\n",
      "train loss:0.04953656761121311\n",
      "train loss:0.022284806245782825\n",
      "train loss:0.10618826301610222\n",
      "train loss:0.07547140481351294\n",
      "train loss:0.06877706408264786\n",
      "train loss:0.04039583434007878\n",
      "train loss:0.08783365360302467\n",
      "train loss:0.028973651961116433\n",
      "train loss:0.021059409549498614\n",
      "train loss:0.10251922514208696\n",
      "train loss:0.07550343638592022\n",
      "train loss:0.03625988355953875\n",
      "train loss:0.07140911189860696\n",
      "train loss:0.07625883843342346\n",
      "train loss:0.08701573837315754\n",
      "train loss:0.05896510341254459\n",
      "train loss:0.049477614348797536\n",
      "train loss:0.06573417114453975\n",
      "train loss:0.06866186406579693\n",
      "train loss:0.023798005377262434\n",
      "train loss:0.07507482586995361\n",
      "train loss:0.041626850096608506\n",
      "train loss:0.11011356150944326\n",
      "train loss:0.07225011348009686\n",
      "train loss:0.12918643129201\n",
      "train loss:0.04140560203014016\n",
      "train loss:0.09826508675931674\n",
      "train loss:0.02479849404179547\n",
      "train loss:0.0858806494830227\n",
      "train loss:0.1977048988608519\n",
      "train loss:0.030345665203690114\n",
      "train loss:0.11866212977191125\n",
      "train loss:0.058738968632297786\n",
      "train loss:0.02772663742613611\n",
      "train loss:0.04213306298876173\n",
      "train loss:0.03217585187917254\n",
      "train loss:0.03642322422845037\n",
      "train loss:0.059111491425438416\n",
      "train loss:0.048554815032863505\n",
      "=== epoch:3, train acc:0.976, test acc:0.982 ===\n",
      "train loss:0.036129362261973996\n",
      "train loss:0.054172853864774176\n",
      "train loss:0.040543489236384914\n",
      "train loss:0.06929479432123634\n",
      "train loss:0.14515587737012292\n",
      "train loss:0.02034487923431415\n",
      "train loss:0.0518295747419216\n",
      "train loss:0.0689446877903076\n",
      "train loss:0.10760530069798113\n",
      "train loss:0.08475809082404911\n",
      "train loss:0.03283273172721739\n",
      "train loss:0.047000403817480337\n",
      "train loss:0.025189975802604524\n",
      "train loss:0.029277329530099774\n",
      "train loss:0.04914064919126618\n",
      "train loss:0.07378602960832158\n",
      "train loss:0.05514709547442037\n",
      "train loss:0.11898514791587068\n",
      "train loss:0.09509701525437658\n",
      "train loss:0.09126438616121968\n",
      "train loss:0.02105043734195543\n",
      "train loss:0.08845670169052353\n",
      "train loss:0.04694156508287043\n",
      "train loss:0.04531674264866299\n",
      "train loss:0.029122129432381752\n",
      "train loss:0.03831534833195085\n",
      "train loss:0.02497638486565885\n",
      "train loss:0.06378211794734767\n",
      "train loss:0.09326864011694525\n",
      "train loss:0.13228748448327304\n",
      "train loss:0.06304738771500747\n",
      "train loss:0.06373599466877457\n",
      "train loss:0.14805533272781285\n",
      "train loss:0.042978471525242916\n",
      "train loss:0.06071804118899154\n",
      "train loss:0.11645771098788335\n",
      "train loss:0.05052645671822573\n",
      "train loss:0.08346385247431522\n",
      "train loss:0.042248287483161555\n",
      "train loss:0.02862405385393678\n",
      "train loss:0.053595657355101245\n",
      "train loss:0.05772895054501867\n",
      "train loss:0.04950272327348199\n",
      "train loss:0.029366700302299407\n",
      "train loss:0.11846045664029133\n",
      "train loss:0.04226483205025088\n",
      "train loss:0.08284988932626958\n",
      "train loss:0.021006616743348142\n",
      "train loss:0.16143773836658043\n",
      "train loss:0.07859073867017967\n",
      "train loss:0.07337498941058923\n",
      "train loss:0.04982125365056512\n",
      "train loss:0.03408490970526973\n",
      "train loss:0.05930981081815851\n",
      "train loss:0.056731792825253785\n",
      "train loss:0.08101523464237022\n",
      "train loss:0.02942458952515976\n",
      "train loss:0.05300749657214879\n",
      "train loss:0.0262175526894155\n",
      "train loss:0.072758686758458\n",
      "train loss:0.042832422394095265\n",
      "train loss:0.05999278835433528\n",
      "train loss:0.04032649883092081\n",
      "train loss:0.10251601635267237\n",
      "train loss:0.09657397122543751\n",
      "train loss:0.060476808894393115\n",
      "train loss:0.06500488419402985\n",
      "train loss:0.043399547827459785\n",
      "train loss:0.09589235288261459\n",
      "train loss:0.04980127214477492\n",
      "train loss:0.020742665140762853\n",
      "train loss:0.0839235530476116\n",
      "train loss:0.05663440416643941\n",
      "train loss:0.05438958610782738\n",
      "train loss:0.014273379667762716\n",
      "train loss:0.03564332459664588\n",
      "train loss:0.05627713910983122\n",
      "train loss:0.07723945325886268\n",
      "train loss:0.08200458885546302\n",
      "train loss:0.0455992554407936\n",
      "train loss:0.06024279580966155\n",
      "train loss:0.04970347602047912\n",
      "train loss:0.06631900462282943\n",
      "train loss:0.0194574603397654\n",
      "train loss:0.10435510491062151\n",
      "train loss:0.11665738956807367\n",
      "train loss:0.06308008664120153\n",
      "train loss:0.0348349618244122\n",
      "train loss:0.033983269501614365\n",
      "train loss:0.08529286030899982\n",
      "train loss:0.03697060242282721\n",
      "train loss:0.048996504050199015\n",
      "train loss:0.028788351513541425\n",
      "train loss:0.08600416122326342\n",
      "train loss:0.07728450038430904\n",
      "train loss:0.031858821136708265\n",
      "train loss:0.08741791636765452\n",
      "train loss:0.04220650397089015\n",
      "train loss:0.04325134981728672\n",
      "train loss:0.06383468385475387\n",
      "train loss:0.15060318832978659\n",
      "train loss:0.044810008403609485\n",
      "train loss:0.0355107399122479\n",
      "train loss:0.023802953172289004\n",
      "train loss:0.019403827598010494\n",
      "train loss:0.14965434705770012\n",
      "train loss:0.06143985460652814\n",
      "train loss:0.09105646075419119\n",
      "train loss:0.0936501570692519\n",
      "train loss:0.03261903144698869\n",
      "train loss:0.12315478646451491\n",
      "train loss:0.08654730517588138\n",
      "train loss:0.027284601467055577\n",
      "train loss:0.025643420407247895\n",
      "train loss:0.06918505451643911\n",
      "train loss:0.026974555588474263\n",
      "train loss:0.09415408421257228\n",
      "train loss:0.05629707176854303\n",
      "train loss:0.15408648528360072\n",
      "train loss:0.022419121333224414\n",
      "train loss:0.06548237911868329\n",
      "train loss:0.046088456304369446\n",
      "train loss:0.03735360010090723\n",
      "train loss:0.04514212282986124\n",
      "train loss:0.028220071092236704\n",
      "train loss:0.08703548240644157\n",
      "train loss:0.05354409731925802\n",
      "train loss:0.024208138939296398\n",
      "train loss:0.04270472722064449\n",
      "train loss:0.04682208147003672\n",
      "train loss:0.047883782238740735\n",
      "train loss:0.08217848702520471\n",
      "train loss:0.062290589841719764\n",
      "train loss:0.019294012697648428\n",
      "train loss:0.045858894939790326\n",
      "train loss:0.15839804878743166\n",
      "train loss:0.05379302177816937\n",
      "train loss:0.17252622418655492\n",
      "train loss:0.04861996962136024\n",
      "train loss:0.1028405537793681\n",
      "train loss:0.07589124422545503\n",
      "train loss:0.1294496266060772\n",
      "train loss:0.033228528418340286\n",
      "train loss:0.05010134230098094\n",
      "train loss:0.02517595824344141\n",
      "train loss:0.0427502449973233\n",
      "train loss:0.03176377883057586\n",
      "train loss:0.06793688989384208\n",
      "train loss:0.05847246550013383\n",
      "train loss:0.01568022493680371\n",
      "train loss:0.020216599030909634\n",
      "train loss:0.057722494117401206\n",
      "train loss:0.0233515796931842\n",
      "train loss:0.030684953992958657\n",
      "train loss:0.03417898355484912\n",
      "train loss:0.03748268478530691\n",
      "train loss:0.13827888458033347\n",
      "train loss:0.07046068854872617\n",
      "train loss:0.11204066092011745\n",
      "train loss:0.041874283042688656\n",
      "train loss:0.131770463864693\n",
      "train loss:0.026315351301849116\n",
      "train loss:0.01666041626453294\n",
      "train loss:0.13308457227950865\n",
      "train loss:0.05078824284194844\n",
      "train loss:0.013926835018821089\n",
      "train loss:0.0865826323039326\n",
      "train loss:0.06376835509350501\n",
      "train loss:0.03693022363789069\n",
      "train loss:0.06440704626346518\n",
      "train loss:0.09837153957316355\n",
      "train loss:0.03703469033117343\n",
      "train loss:0.049246784921747805\n",
      "train loss:0.03525753738824309\n",
      "train loss:0.1450253963387936\n",
      "train loss:0.04553948222686088\n",
      "train loss:0.018560387481468824\n",
      "train loss:0.03550139146260831\n",
      "train loss:0.17111581241827437\n",
      "train loss:0.032964811431519456\n",
      "train loss:0.05097357549359912\n",
      "train loss:0.03549030689977017\n",
      "train loss:0.03486041664497776\n",
      "train loss:0.13482975698364436\n",
      "train loss:0.07121285655397483\n",
      "train loss:0.08596652991293295\n",
      "train loss:0.019880883016231164\n",
      "train loss:0.05637879430770298\n",
      "train loss:0.0739558283956812\n",
      "train loss:0.07427398163288346\n",
      "train loss:0.15503397801847119\n",
      "train loss:0.05610050302040743\n",
      "train loss:0.031952790770899984\n",
      "train loss:0.013696852634739241\n",
      "train loss:0.10078727002926308\n",
      "train loss:0.03341367183813587\n",
      "train loss:0.06742139529166465\n",
      "train loss:0.022747537744415146\n",
      "train loss:0.07710186587342503\n",
      "train loss:0.020859646320651737\n",
      "train loss:0.04019406921410722\n",
      "train loss:0.04115399688870176\n",
      "train loss:0.10960800240690598\n",
      "train loss:0.029317421998558798\n",
      "train loss:0.09155337869685697\n",
      "train loss:0.11061486711492091\n",
      "train loss:0.08038580795991784\n",
      "train loss:0.029514750215476865\n",
      "train loss:0.05457959322178537\n",
      "train loss:0.030959413479590907\n",
      "train loss:0.05605769320698695\n",
      "train loss:0.09410468442996128\n",
      "train loss:0.011889088158814341\n",
      "train loss:0.0342197237473998\n",
      "train loss:0.05014404647323947\n",
      "train loss:0.04052565515553166\n",
      "train loss:0.08694248165013961\n",
      "train loss:0.055197152536818124\n",
      "train loss:0.06073508645161143\n",
      "train loss:0.033161388803125566\n",
      "train loss:0.06801901908595176\n",
      "train loss:0.035080116240977725\n",
      "train loss:0.05853779777236876\n",
      "train loss:0.10395779963207383\n",
      "train loss:0.0216194836258855\n",
      "train loss:0.03950184916930031\n",
      "train loss:0.19157452770218744\n",
      "train loss:0.11239980946311441\n",
      "train loss:0.09871618927914909\n",
      "train loss:0.12023978108804488\n",
      "train loss:0.030140744325621714\n",
      "train loss:0.023887230896482113\n",
      "train loss:0.02562463005417349\n",
      "train loss:0.0729365207918044\n",
      "train loss:0.051813487670485915\n",
      "train loss:0.06386630378083695\n",
      "train loss:0.018401724780066762\n",
      "train loss:0.032388754617787174\n",
      "train loss:0.08303315976743264\n",
      "train loss:0.048990942882932244\n",
      "train loss:0.034840896971879504\n",
      "train loss:0.06876044955756604\n",
      "train loss:0.016368631082221526\n",
      "train loss:0.07219508261060893\n",
      "train loss:0.034953901420590155\n",
      "train loss:0.07249369846127572\n",
      "train loss:0.033480648983827875\n",
      "train loss:0.03431619640817874\n",
      "train loss:0.033437034268599716\n",
      "train loss:0.04129358351104471\n",
      "train loss:0.047186025107948906\n",
      "train loss:0.0730146943868022\n",
      "train loss:0.09119192815258952\n",
      "train loss:0.05295874960561899\n",
      "train loss:0.02184732207850896\n",
      "train loss:0.057101441400166174\n",
      "train loss:0.05766737086807101\n",
      "train loss:0.04423226667354308\n",
      "train loss:0.03846639835534812\n",
      "train loss:0.04940570331683517\n",
      "train loss:0.02512808525155664\n",
      "train loss:0.033907163726319384\n",
      "train loss:0.01578906689887535\n",
      "train loss:0.09550154346095288\n",
      "train loss:0.07612669504668855\n",
      "train loss:0.04023660615527387\n",
      "train loss:0.04710371319745692\n",
      "train loss:0.10364438436912643\n",
      "train loss:0.05551061309202202\n",
      "train loss:0.02619655028101762\n",
      "train loss:0.05554638147344454\n",
      "train loss:0.044425500445793166\n",
      "train loss:0.043600649165690744\n",
      "train loss:0.05209569377370299\n",
      "train loss:0.0722899823482022\n",
      "train loss:0.0914642208556659\n",
      "train loss:0.02543823780801867\n",
      "train loss:0.12587658032569093\n",
      "train loss:0.056257952364031354\n",
      "train loss:0.06469464777430063\n",
      "train loss:0.028263570667212218\n",
      "train loss:0.022748613542235892\n",
      "train loss:0.008582618050813557\n",
      "train loss:0.08317155308703424\n",
      "train loss:0.08072889245659638\n",
      "train loss:0.062051299592126324\n",
      "train loss:0.02360333509331268\n",
      "train loss:0.07320513244537427\n",
      "train loss:0.04042326106477046\n",
      "train loss:0.01128932205187054\n",
      "train loss:0.036753103384376394\n",
      "train loss:0.061372971538875755\n",
      "train loss:0.1266143316668098\n",
      "train loss:0.023896971369230004\n",
      "train loss:0.07670317482544547\n",
      "train loss:0.03299333034491446\n",
      "train loss:0.031224804417910788\n",
      "train loss:0.02598916697851339\n",
      "train loss:0.04090838925452532\n",
      "train loss:0.05263037370954428\n",
      "train loss:0.04609113081664881\n",
      "train loss:0.01887698342738762\n",
      "train loss:0.059188560702598904\n",
      "train loss:0.009918691217072059\n",
      "train loss:0.050736224398036545\n",
      "train loss:0.030683261285726685\n",
      "train loss:0.08137419920275711\n",
      "train loss:0.05196326487797691\n",
      "train loss:0.041060554023880834\n",
      "train loss:0.04097850946934861\n",
      "train loss:0.04831946088770287\n",
      "train loss:0.03534795564520344\n",
      "train loss:0.05555784716592174\n",
      "train loss:0.04278070993154912\n",
      "train loss:0.032665684010349263\n",
      "train loss:0.023547290622461237\n",
      "train loss:0.09873932973628609\n",
      "train loss:0.09139575225406736\n",
      "train loss:0.021792092250081642\n",
      "train loss:0.008661914167191385\n",
      "train loss:0.10769472221419976\n",
      "train loss:0.11001838035044431\n",
      "train loss:0.04588109711885502\n",
      "train loss:0.056517348720108114\n",
      "train loss:0.047328656626394634\n",
      "train loss:0.02287906233142309\n",
      "train loss:0.012329658148462718\n",
      "train loss:0.09431033510038463\n",
      "train loss:0.049387969426159695\n",
      "train loss:0.032043851082765196\n",
      "train loss:0.03760675273060687\n",
      "train loss:0.04155999661229114\n",
      "train loss:0.11676449408649237\n",
      "train loss:0.09998194578709961\n",
      "train loss:0.030453031484729352\n",
      "train loss:0.14323678930959707\n",
      "train loss:0.04098991727607415\n",
      "train loss:0.09538681835907586\n",
      "train loss:0.06712178992197079\n",
      "train loss:0.0903456923217918\n",
      "train loss:0.0344803204659586\n",
      "train loss:0.029305251238860033\n",
      "train loss:0.012057150119059857\n",
      "train loss:0.07899560369694704\n",
      "train loss:0.09301105638833798\n",
      "train loss:0.05395429577632194\n",
      "train loss:0.08523097806057751\n",
      "train loss:0.018148206195337507\n",
      "train loss:0.027858406131183823\n",
      "train loss:0.07715902542196615\n",
      "train loss:0.045778717604765394\n",
      "train loss:0.025363734854672883\n",
      "train loss:0.049679220900265414\n",
      "train loss:0.07168142025743682\n",
      "train loss:0.05349882902250073\n",
      "train loss:0.055955087755258016\n",
      "train loss:0.02106205744487706\n",
      "train loss:0.04918752024195446\n",
      "train loss:0.1106091086816709\n",
      "train loss:0.020145869342125255\n",
      "train loss:0.0707124135908051\n",
      "train loss:0.04095439205006292\n",
      "train loss:0.062239059457225576\n",
      "train loss:0.016573964685045506\n",
      "train loss:0.16137498004632164\n",
      "train loss:0.07200854033237407\n",
      "train loss:0.07816209780412517\n",
      "train loss:0.018858404371661502\n",
      "train loss:0.029716414265973042\n",
      "train loss:0.08081124708924137\n",
      "train loss:0.10108468967422167\n",
      "train loss:0.03874223341945409\n",
      "train loss:0.02162941469205856\n",
      "train loss:0.0829465859757729\n",
      "train loss:0.035302497117821435\n",
      "train loss:0.13531091644459559\n",
      "train loss:0.05244730066700236\n",
      "train loss:0.02382493672623093\n",
      "train loss:0.030804046286157002\n",
      "train loss:0.05607948684487714\n",
      "train loss:0.08247476439084368\n",
      "train loss:0.02882633709348635\n",
      "train loss:0.027447801980718625\n",
      "train loss:0.04730000868004664\n",
      "train loss:0.04076430208014989\n",
      "train loss:0.060847922308897885\n",
      "train loss:0.035429143339934216\n",
      "train loss:0.030003521000651078\n",
      "train loss:0.014845459839502036\n",
      "train loss:0.04111662214945131\n",
      "train loss:0.03101464745821156\n",
      "train loss:0.07719741021949944\n",
      "train loss:0.009882447254064307\n",
      "train loss:0.016146451852742962\n",
      "train loss:0.015209625541095497\n",
      "train loss:0.05091388324900911\n",
      "train loss:0.08855001648649029\n",
      "train loss:0.017011605577806236\n",
      "train loss:0.023897117607750675\n",
      "train loss:0.04760481228196211\n",
      "train loss:0.04189094702048887\n",
      "train loss:0.028444431509812695\n",
      "train loss:0.05724035650846882\n",
      "train loss:0.012670211340230352\n",
      "train loss:0.021118749971972287\n",
      "train loss:0.02417283096603895\n",
      "train loss:0.024395852470802715\n",
      "train loss:0.09024267934670681\n",
      "train loss:0.027278995728274098\n",
      "train loss:0.08696430016555351\n",
      "train loss:0.023935709169899667\n",
      "train loss:0.08467138357439995\n",
      "train loss:0.06176590420738754\n",
      "train loss:0.042479121237274604\n",
      "train loss:0.15895354815205592\n",
      "train loss:0.08374905617748553\n",
      "train loss:0.03195561922958679\n",
      "train loss:0.021334021349086428\n",
      "train loss:0.11565849626264883\n",
      "train loss:0.06326419369208475\n",
      "train loss:0.04802316239498003\n",
      "train loss:0.05276554639742782\n",
      "train loss:0.021495215722977964\n",
      "train loss:0.07156470165562917\n",
      "train loss:0.017801044916373934\n",
      "train loss:0.022510643368829436\n",
      "train loss:0.07209057082355937\n",
      "train loss:0.07210250386527466\n",
      "train loss:0.05186179520578321\n",
      "train loss:0.04457401470676116\n",
      "train loss:0.056313189932824574\n",
      "train loss:0.03909089116235597\n",
      "train loss:0.05939550835504842\n",
      "train loss:0.030923818336170285\n",
      "train loss:0.08169109408595826\n",
      "train loss:0.07053749857025543\n",
      "train loss:0.030967357894194407\n",
      "train loss:0.0799533087994062\n",
      "train loss:0.09474591486806495\n",
      "train loss:0.04119403327845519\n",
      "train loss:0.02075987310419878\n",
      "train loss:0.061106451803406764\n",
      "train loss:0.014791373283858577\n",
      "train loss:0.035138847475717216\n",
      "train loss:0.08285328575865927\n",
      "train loss:0.04588926976893358\n",
      "train loss:0.013137649763987083\n",
      "train loss:0.050199564371020404\n",
      "train loss:0.03962299400797861\n",
      "train loss:0.024129750096261126\n",
      "train loss:0.0780327063113415\n",
      "train loss:0.060903306573109335\n",
      "train loss:0.06193083525000322\n",
      "train loss:0.01906941665826431\n",
      "train loss:0.06783888036335851\n",
      "train loss:0.06096701156730644\n",
      "train loss:0.03085936853461389\n",
      "train loss:0.04031069516664314\n",
      "train loss:0.08941752785738116\n",
      "train loss:0.06562755378916715\n",
      "train loss:0.0497600266892488\n",
      "train loss:0.026952719275779104\n",
      "train loss:0.017666095244229203\n",
      "train loss:0.06373384999003276\n",
      "train loss:0.042316885299987714\n",
      "train loss:0.05757765624702639\n",
      "train loss:0.04581203058593387\n",
      "train loss:0.03428641180807775\n",
      "train loss:0.033740026779324465\n",
      "train loss:0.19966757693380033\n",
      "train loss:0.05081104956917552\n",
      "train loss:0.04893158318974649\n",
      "train loss:0.022874903933769796\n",
      "train loss:0.07155781250374507\n",
      "train loss:0.04530018026320648\n",
      "train loss:0.011625040856077844\n",
      "train loss:0.04918758526270194\n",
      "train loss:0.05783546531596975\n",
      "train loss:0.09254646957382574\n",
      "train loss:0.016647519267170178\n",
      "train loss:0.15670912928942024\n",
      "train loss:0.05525363685037919\n",
      "train loss:0.06265168512815814\n",
      "train loss:0.021720015200085527\n",
      "train loss:0.013907040181348038\n",
      "train loss:0.06665321159961538\n",
      "train loss:0.07271780819956866\n",
      "train loss:0.0883927308883385\n",
      "train loss:0.035322175109864476\n",
      "train loss:0.0427896121162572\n",
      "train loss:0.06088899339182756\n",
      "train loss:0.03243269133243584\n",
      "train loss:0.03672135268793556\n",
      "train loss:0.026823828701474543\n",
      "train loss:0.024713427248154552\n",
      "train loss:0.04842404824290506\n",
      "train loss:0.07996638526060088\n",
      "train loss:0.060696364766471785\n",
      "train loss:0.04935184288209042\n",
      "train loss:0.03825491887135049\n",
      "train loss:0.11326493173327981\n",
      "train loss:0.006484473226631394\n",
      "train loss:0.023132927032016637\n",
      "train loss:0.09348086793265882\n",
      "train loss:0.039092815577957955\n",
      "train loss:0.047288814667982154\n",
      "train loss:0.02967227449057514\n",
      "train loss:0.029403215778753697\n",
      "train loss:0.013318510384971713\n",
      "train loss:0.019008416319481355\n",
      "train loss:0.01853052486840287\n",
      "train loss:0.03607629562353479\n",
      "train loss:0.01838316392725877\n",
      "train loss:0.019946663128237868\n",
      "train loss:0.054809848015333305\n",
      "train loss:0.03626830345799401\n",
      "train loss:0.04599461524039432\n",
      "train loss:0.024926036003771333\n",
      "train loss:0.019767698449983377\n",
      "train loss:0.044098846131445414\n",
      "train loss:0.07119326013161785\n",
      "train loss:0.06568304847996463\n",
      "train loss:0.014743561831186736\n",
      "train loss:0.02620684406251751\n",
      "train loss:0.027548374609025657\n",
      "train loss:0.041802013318678034\n",
      "train loss:0.07212218135013951\n",
      "train loss:0.0165950100040876\n",
      "train loss:0.034620928501451015\n",
      "train loss:0.020521560248167522\n",
      "train loss:0.037467802413456444\n",
      "train loss:0.04568159101553931\n",
      "train loss:0.0372102333600578\n",
      "train loss:0.0442269762109456\n",
      "train loss:0.08783582006623047\n",
      "train loss:0.013119518235580972\n",
      "train loss:0.01504303853840421\n",
      "train loss:0.009842295026141524\n",
      "train loss:0.04045970113513182\n",
      "train loss:0.037019478790422625\n",
      "train loss:0.05767931475636825\n",
      "train loss:0.017037078598605374\n",
      "train loss:0.01711078894879494\n",
      "train loss:0.07313436125700339\n",
      "train loss:0.039247136432235895\n",
      "train loss:0.011397600863331924\n",
      "train loss:0.09115604089117323\n",
      "train loss:0.00724618796616154\n",
      "train loss:0.10815219018176293\n",
      "train loss:0.07078501610424252\n",
      "train loss:0.013622916733038505\n",
      "train loss:0.04013845736039115\n",
      "train loss:0.09288131203514258\n",
      "train loss:0.01881375206109895\n",
      "train loss:0.0637236397744437\n",
      "train loss:0.017584190412525035\n",
      "train loss:0.039174892715602413\n",
      "train loss:0.05115735519148135\n",
      "train loss:0.025745984111878468\n",
      "train loss:0.05995176908630774\n",
      "train loss:0.03291733485384335\n",
      "train loss:0.04129647607669665\n",
      "train loss:0.07364604584041609\n",
      "train loss:0.022031237879258713\n",
      "train loss:0.008559540177432486\n",
      "train loss:0.032324486111797494\n",
      "train loss:0.015138195008667593\n",
      "train loss:0.05698762623578631\n",
      "train loss:0.055137163437036465\n",
      "train loss:0.0055191262105720755\n",
      "train loss:0.030535677948863728\n",
      "train loss:0.10056735881831419\n",
      "train loss:0.06589351383630639\n",
      "train loss:0.04438954874043262\n",
      "train loss:0.021261421289192596\n",
      "train loss:0.0304893678477936\n",
      "train loss:0.03793956255624028\n",
      "train loss:0.0056368694435141075\n",
      "train loss:0.04871596234525266\n",
      "train loss:0.026961972730422314\n",
      "train loss:0.006955597725222858\n",
      "train loss:0.07335132725865816\n",
      "train loss:0.013752779677279116\n",
      "train loss:0.011499746382882557\n",
      "train loss:0.009498402027027146\n",
      "train loss:0.0678072143267943\n",
      "train loss:0.018642353445734217\n",
      "train loss:0.0065213403634690635\n",
      "train loss:0.019632516790317734\n",
      "train loss:0.09307253800101037\n",
      "train loss:0.04051536656413396\n",
      "train loss:0.012100842444308264\n",
      "train loss:0.03196934338121188\n",
      "train loss:0.03222599439739321\n",
      "train loss:0.04667342950375922\n",
      "train loss:0.03435768726715728\n",
      "train loss:0.06826831905572833\n",
      "train loss:0.05460411497672542\n",
      "train loss:0.03754584376677127\n",
      "train loss:0.030832178158478555\n",
      "=== epoch:4, train acc:0.983, test acc:0.979 ===\n",
      "train loss:0.020247970879158073\n",
      "train loss:0.013682788073740682\n",
      "train loss:0.027066049460280607\n",
      "train loss:0.09636022737054707\n",
      "train loss:0.05128130376945156\n",
      "train loss:0.12161789220388015\n",
      "train loss:0.01565182098973773\n",
      "train loss:0.09167024891193204\n",
      "train loss:0.025681651877087187\n",
      "train loss:0.06459514818210799\n",
      "train loss:0.03087355654071663\n",
      "train loss:0.04532798486645751\n",
      "train loss:0.08019990688125611\n",
      "train loss:0.059667470457707344\n",
      "train loss:0.023779777853840032\n",
      "train loss:0.10806652100770213\n",
      "train loss:0.01617733440180954\n",
      "train loss:0.08688033753126122\n",
      "train loss:0.025608585938690913\n",
      "train loss:0.019940957353794943\n",
      "train loss:0.015420634916265224\n",
      "train loss:0.07761760739797509\n",
      "train loss:0.020093769117493082\n",
      "train loss:0.018135562085582903\n",
      "train loss:0.042736929178576286\n",
      "train loss:0.043644735801762896\n",
      "train loss:0.07177151492222478\n",
      "train loss:0.018819948913492146\n",
      "train loss:0.1254151072320751\n",
      "train loss:0.019155447825434786\n",
      "train loss:0.19894374035990445\n",
      "train loss:0.0768957691401011\n",
      "train loss:0.06733742114195923\n",
      "train loss:0.05470429741366483\n",
      "train loss:0.07917761164569238\n",
      "train loss:0.017694072395362682\n",
      "train loss:0.014428554937114311\n",
      "train loss:0.014063582459218871\n",
      "train loss:0.035107441353641994\n",
      "train loss:0.062459804416875236\n",
      "train loss:0.06058828380780303\n",
      "train loss:0.07411783905585938\n",
      "train loss:0.04556336796456148\n",
      "train loss:0.04294449755566827\n",
      "train loss:0.03204248356333346\n",
      "train loss:0.10001808331230645\n",
      "train loss:0.011222703779479702\n",
      "train loss:0.028669137914748646\n",
      "train loss:0.05829483309688358\n",
      "train loss:0.1290220845115127\n",
      "train loss:0.031395472188044685\n",
      "train loss:0.02338618862730476\n",
      "train loss:0.03677930954417518\n",
      "train loss:0.06940890310393506\n",
      "train loss:0.06912446886777145\n",
      "train loss:0.05175837789305952\n",
      "train loss:0.03319312790388579\n",
      "train loss:0.013742073209646035\n",
      "train loss:0.05642231879903943\n",
      "train loss:0.07634112645409949\n",
      "train loss:0.15705785071856732\n",
      "train loss:0.008262187614647138\n",
      "train loss:0.041203737675669094\n",
      "train loss:0.05458900470523713\n",
      "train loss:0.03290347914642381\n",
      "train loss:0.22819405574748766\n",
      "train loss:0.04434048173090915\n",
      "train loss:0.08884462695737144\n",
      "train loss:0.019729933622834903\n",
      "train loss:0.030004613797180987\n",
      "train loss:0.04247794023068224\n",
      "train loss:0.02753214154997801\n",
      "train loss:0.0091264156276279\n",
      "train loss:0.04637478120446046\n",
      "train loss:0.013987888043415064\n",
      "train loss:0.07891828877954465\n",
      "train loss:0.015486817179544183\n",
      "train loss:0.02903557450836887\n",
      "train loss:0.02238561874934291\n",
      "train loss:0.055911533183631956\n",
      "train loss:0.0574404688888861\n",
      "train loss:0.015664864705793875\n",
      "train loss:0.017565867231779123\n",
      "train loss:0.08425335626010337\n",
      "train loss:0.017832314859582418\n",
      "train loss:0.006015213490334214\n",
      "train loss:0.06779577493566886\n",
      "train loss:0.01053573364588033\n",
      "train loss:0.08490530571580347\n",
      "train loss:0.05686335798839526\n",
      "train loss:0.09590065155820955\n",
      "train loss:0.05814017220326579\n",
      "train loss:0.010820669015441509\n",
      "train loss:0.018367373308445775\n",
      "train loss:0.030323387515529543\n",
      "train loss:0.028901319038298897\n",
      "train loss:0.047785572173002\n",
      "train loss:0.04469438227500954\n",
      "train loss:0.07667186828192914\n",
      "train loss:0.05528534226867501\n",
      "train loss:0.06129506805962975\n",
      "train loss:0.06009985103089521\n",
      "train loss:0.03715534499658553\n",
      "train loss:0.04766297185585267\n",
      "train loss:0.035311131927548124\n",
      "train loss:0.06588378914195514\n",
      "train loss:0.0381762992972402\n",
      "train loss:0.024260931467777858\n",
      "train loss:0.03315931031240712\n",
      "train loss:0.013776564253961461\n",
      "train loss:0.04612501065245769\n",
      "train loss:0.02308140959980129\n",
      "train loss:0.10422719309875562\n",
      "train loss:0.032359116454283236\n",
      "train loss:0.05236657335238891\n",
      "train loss:0.12727702635152985\n",
      "train loss:0.07941883821539845\n",
      "train loss:0.04190172690077554\n",
      "train loss:0.027426359110342798\n",
      "train loss:0.09204721583952309\n",
      "train loss:0.07982890999445123\n",
      "train loss:0.04766674898980365\n",
      "train loss:0.03997920431287731\n",
      "train loss:0.020629323241135835\n",
      "train loss:0.058312742436841036\n",
      "train loss:0.03707259988757814\n",
      "train loss:0.028866215087111628\n",
      "train loss:0.03268568038679219\n",
      "train loss:0.08444281659409951\n",
      "train loss:0.1247186194982461\n",
      "train loss:0.0405664654809721\n",
      "train loss:0.05445362902171893\n",
      "train loss:0.08515518745235466\n",
      "train loss:0.06199669320197408\n",
      "train loss:0.018743774836530603\n",
      "train loss:0.015782276279285003\n",
      "train loss:0.03405747224506573\n",
      "train loss:0.02228664265670922\n",
      "train loss:0.018209659006708984\n",
      "train loss:0.013719530436451046\n",
      "train loss:0.08399730090203081\n",
      "train loss:0.03262354002644899\n",
      "train loss:0.02503379251250523\n",
      "train loss:0.06907564776610378\n",
      "train loss:0.0727724472217234\n",
      "train loss:0.05829591578253199\n",
      "train loss:0.008166823320618008\n",
      "train loss:0.03305757600480588\n",
      "train loss:0.07431082585023954\n",
      "train loss:0.02886391058003877\n",
      "train loss:0.018550490366062475\n",
      "train loss:0.07464008936153343\n",
      "train loss:0.08775082479638213\n",
      "train loss:0.06874101188042805\n",
      "train loss:0.0515451400777853\n",
      "train loss:0.028873147584716704\n",
      "train loss:0.053174801644535565\n",
      "train loss:0.029996895858144138\n",
      "train loss:0.029220689181027323\n",
      "train loss:0.012664295490539792\n",
      "train loss:0.034013791501889074\n",
      "train loss:0.021294587414696697\n",
      "train loss:0.0316567533235066\n",
      "train loss:0.031777158611465045\n",
      "train loss:0.03069544703588028\n",
      "train loss:0.029008445002007104\n",
      "train loss:0.0280390792849869\n",
      "train loss:0.01791512069495397\n",
      "train loss:0.06827514726518308\n",
      "train loss:0.02578010807641329\n",
      "train loss:0.05202197033724386\n",
      "train loss:0.06707989429935027\n",
      "train loss:0.04287888885073301\n",
      "train loss:0.021106312913918436\n",
      "train loss:0.03354733829076358\n",
      "train loss:0.021069720402050258\n",
      "train loss:0.09491473249771654\n",
      "train loss:0.008296383247653945\n",
      "train loss:0.10068650351661511\n",
      "train loss:0.017948408009387615\n",
      "train loss:0.011897263460372781\n",
      "train loss:0.030238589036203755\n",
      "train loss:0.01627304570940831\n",
      "train loss:0.06436669105031184\n",
      "train loss:0.015067763124625242\n",
      "train loss:0.0564183457146824\n",
      "train loss:0.10288463403642677\n",
      "train loss:0.01992884060576626\n",
      "train loss:0.03147550929563529\n",
      "train loss:0.025252696338839854\n",
      "train loss:0.05069492516474541\n",
      "train loss:0.040006337987115254\n",
      "train loss:0.061827236823326845\n",
      "train loss:0.02240279395214492\n",
      "train loss:0.007160073700994862\n",
      "train loss:0.04415863824813719\n",
      "train loss:0.02016805527718736\n",
      "train loss:0.012538183193327319\n",
      "train loss:0.03441554097931848\n",
      "train loss:0.0384533511881645\n",
      "train loss:0.012508951535849737\n",
      "train loss:0.061225468352619\n",
      "train loss:0.03566046764162936\n",
      "train loss:0.007030145054304286\n",
      "train loss:0.08600644662364611\n",
      "train loss:0.009548956933304526\n",
      "train loss:0.010996234741723558\n",
      "train loss:0.026225912686477323\n",
      "train loss:0.007889496481537831\n",
      "train loss:0.08693659810994181\n",
      "train loss:0.010796594202748993\n",
      "train loss:0.024378119487590864\n",
      "train loss:0.07435420878673897\n",
      "train loss:0.040957557522270926\n",
      "train loss:0.027213775785982438\n",
      "train loss:0.02891596906017583\n",
      "train loss:0.009263408107777073\n",
      "train loss:0.029017996969937374\n",
      "train loss:0.04707626473713012\n",
      "train loss:0.005209598592833513\n",
      "train loss:0.06893628439666338\n",
      "train loss:0.03594492202267408\n",
      "train loss:0.08442938692138903\n",
      "train loss:0.0406122986701665\n",
      "train loss:0.05540207137872206\n",
      "train loss:0.033382588411844896\n",
      "train loss:0.0151712207043043\n",
      "train loss:0.030541404712376118\n",
      "train loss:0.042652605266052246\n",
      "train loss:0.04149280539237522\n",
      "train loss:0.018088343576998043\n",
      "train loss:0.0432987395377852\n",
      "train loss:0.05055255017373989\n",
      "train loss:0.025222747866094228\n",
      "train loss:0.03165167416166549\n",
      "train loss:0.027675474505959786\n",
      "train loss:0.006604250021382714\n",
      "train loss:0.018201920759792253\n",
      "train loss:0.0311744901566884\n",
      "train loss:0.06060158125325736\n",
      "train loss:0.05792022668061774\n",
      "train loss:0.017537178092528895\n",
      "train loss:0.05322044104111307\n",
      "train loss:0.008890651760205447\n",
      "train loss:0.03365433461518021\n",
      "train loss:0.01567990234598917\n",
      "train loss:0.02120088271897512\n",
      "train loss:0.01668586270676326\n",
      "train loss:0.05622849259235104\n",
      "train loss:0.05698074348411067\n",
      "train loss:0.0626629940435424\n",
      "train loss:0.011983782030180003\n",
      "train loss:0.019189108642757764\n",
      "train loss:0.04595759625689219\n",
      "train loss:0.008154150528096895\n",
      "train loss:0.05736605522857756\n",
      "train loss:0.06585992408718273\n",
      "train loss:0.1781122968251877\n",
      "train loss:0.08706983707537247\n",
      "train loss:0.015276606734553111\n",
      "train loss:0.01062571292279038\n",
      "train loss:0.04371699029691488\n",
      "train loss:0.024422344654967284\n",
      "train loss:0.008272332941233107\n",
      "train loss:0.01205632745720441\n",
      "train loss:0.025036091031291247\n",
      "train loss:0.00858151188225471\n",
      "train loss:0.03233847169712195\n",
      "train loss:0.01691804869063856\n",
      "train loss:0.028814508556363076\n",
      "train loss:0.02545089533426247\n",
      "train loss:0.03932404926254553\n",
      "train loss:0.07522213238372188\n",
      "train loss:0.04857952995039486\n",
      "train loss:0.03733217670726538\n",
      "train loss:0.011536286965405014\n",
      "train loss:0.002891125713238835\n",
      "train loss:0.010152671754527378\n",
      "train loss:0.05346770526604934\n",
      "train loss:0.0074340959898299966\n",
      "train loss:0.03292900947065767\n",
      "train loss:0.04374498843530948\n",
      "train loss:0.01421829472293976\n",
      "train loss:0.013450578873392086\n",
      "train loss:0.09644943655884745\n",
      "train loss:0.011433404070496079\n",
      "train loss:0.02199118565502164\n",
      "train loss:0.026126392269720754\n",
      "train loss:0.06091895896116049\n",
      "train loss:0.025074986397063178\n",
      "train loss:0.017131421056343795\n",
      "train loss:0.03430526608964341\n",
      "train loss:0.033183833867474075\n",
      "train loss:0.028215229023768395\n",
      "train loss:0.0849685811890761\n",
      "train loss:0.0643091863601865\n",
      "train loss:0.020265041810187446\n",
      "train loss:0.06891638682381149\n",
      "train loss:0.04777633721082765\n",
      "train loss:0.017895435341153187\n",
      "train loss:0.03546559568697672\n",
      "train loss:0.04501871355052609\n",
      "train loss:0.02395452518076411\n",
      "train loss:0.011415187715969857\n",
      "train loss:0.016884370233687634\n",
      "train loss:0.0626737447595117\n",
      "train loss:0.02808235123425212\n",
      "train loss:0.026284485411780828\n",
      "train loss:0.035061091934929844\n",
      "train loss:0.03228800286813947\n",
      "train loss:0.02907739615332692\n",
      "train loss:0.0580139379295678\n",
      "train loss:0.01052621759967027\n",
      "train loss:0.04918289410045861\n",
      "train loss:0.07665756354892576\n",
      "train loss:0.04517088096266593\n",
      "train loss:0.0100422164701639\n",
      "train loss:0.005686897738536141\n",
      "train loss:0.014504778982905702\n",
      "train loss:0.03271255109411289\n",
      "train loss:0.01545243438562087\n",
      "train loss:0.03258687861762291\n",
      "train loss:0.022223687671279677\n",
      "train loss:0.0421734314628247\n",
      "train loss:0.055618623986032276\n",
      "train loss:0.07591526793432944\n",
      "train loss:0.03519985909186671\n",
      "train loss:0.09058998744241466\n",
      "train loss:0.052780416942310546\n",
      "train loss:0.03329732313332281\n",
      "train loss:0.07020675794087704\n",
      "train loss:0.07531302473913916\n",
      "train loss:0.03189381819145878\n",
      "train loss:0.020593645055421425\n",
      "train loss:0.07808306118949662\n",
      "train loss:0.008326443193533676\n",
      "train loss:0.010466912806278957\n",
      "train loss:0.004365861711216531\n",
      "train loss:0.04060212844876673\n",
      "train loss:0.02948945607354939\n",
      "train loss:0.051826571372046694\n",
      "train loss:0.01486883391012689\n",
      "train loss:0.01573121910952558\n",
      "train loss:0.03490398995166141\n",
      "train loss:0.014359983957088542\n",
      "train loss:0.035090974524304985\n",
      "train loss:0.02280111099107164\n",
      "train loss:0.013194002369878888\n",
      "train loss:0.03912261493644811\n",
      "train loss:0.010359395752644482\n",
      "train loss:0.008258928830323189\n",
      "train loss:0.06960118288544785\n",
      "train loss:0.01092430564543869\n",
      "train loss:0.004416634753442106\n",
      "train loss:0.013927473041764806\n",
      "train loss:0.005851172373994938\n",
      "train loss:0.012186964108060874\n",
      "train loss:0.03971819656171882\n",
      "train loss:0.03066484057036557\n",
      "train loss:0.08333031211995656\n",
      "train loss:0.010114815955284932\n",
      "train loss:0.08277473778439502\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e4df5-a92a-48a6-af68-9b80e56aa5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
