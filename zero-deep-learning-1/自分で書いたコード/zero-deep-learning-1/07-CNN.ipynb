{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a3e6159-f9fa-4640-9b89-1fd9456ccf7b",
   "metadata": {},
   "source": [
    "# 第7章 畳み込みニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e5ea42-5429-4802-b13d-28d308edc08e",
   "metadata": {},
   "source": [
    "* ## 7.4 Convolution / Poolingレイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b9081-5bdd-4b52-8f07-cbb50bad26ac",
   "metadata": {},
   "source": [
    "#### CNNでは、各層を流れるデータは四次元のデータになる。例えば、データの形状が(10, 1, 28, 28)だと、高さ28横幅28、1チャンネルのデータが10個存在しているのと同じ。\n",
    "#### 普通にfor文で実装するとえらい時間がかかるので、im2colを用いてデータを展開することで、高速に実装する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3e204c-0a46-425a-99e7-1ea087f5ccf8",
   "metadata": {},
   "source": [
    "* ## im2col関数の引数\n",
    "### im2col(input_data, filter_h, filter_w, stride=1, pad=0)\n",
    "\n",
    "#### input_data : (データ数、チャンネル、高さ、横幅）の四次元配列からなる入力データ\n",
    "#### filter_h : フィルターの高さ\n",
    "#### filter_w : フィルターの横幅\n",
    "#### stride : ストライド\n",
    "#### pad : パディング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63001553-13a1-4ee1-9659-0ce0e284b0be",
   "metadata": {},
   "source": [
    "* ## Convolutionレイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cc683b6-9339-448d-9bdc-c789e20de91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n"
     ]
    }
   ],
   "source": [
    "# im2colの動作確認\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.util import im2col\n",
    "\n",
    "# データ数1, ３チャンネル、 高さと横幅7のデータを用意\n",
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "821cec3b-6890-41b3-932a-8655946d484b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "# バッチサイズ（データサイズ）を１０倍にする\n",
    "# データサイズが10倍なので縦（行）要素も10倍になった。\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a9954d5-4a3e-4259-ad7c-c5ea55e0fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 畳み込み層\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape # フィルター\n",
    "        N, C, H, W = x.shape # データ\n",
    "        out_h = int(1 + (H + 2*self.pad -FH) / self.stride) # 出力データの高さ\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride) # 出力データの横幅\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        # reshapeの引数に-1を渡すといい感じに整形してくれる\n",
    "        col_W = self.W.reshape(FN, -1).T # フィルターの展開\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "\n",
    "        # transposeで軸の順番を入れ替える\n",
    "        # 変更前は（Num, Height, Width, Channel）の順なので、入力データと同様に\n",
    "        # （Num, Channel, Height, Wirth）の順番に入れ替える\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b292f-9c2a-41f9-a8ff-9b6146c68e5f",
   "metadata": {},
   "source": [
    "* ## Poolingレイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2b16420-745c-4502-8a34-efb50cbc2d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=2, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride) # 出力データの高さ\n",
    "        out_w = int(1 + (W + self.pool_w) / self.stride) # 出力データの横幅\n",
    "\n",
    "        # （１）展開\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "\n",
    "        # （２）最大値を抜き出す\n",
    "        out = np.max(col, axis=1)\n",
    "        # （３）整形\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7de7bed-428f-4707-83e9-6354f20b70a9",
   "metadata": {},
   "source": [
    "* # 7.5 CNNの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60eb26e1-e966-4607-8519-50d2ec362213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"単純なConvNet\n",
    "\n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 入力サイズ（MNISTの場合は784）\n",
    "    hidden_size_list : 隠れ層のニューロンの数のリスト（e.g. [100, 100, 100]）\n",
    "    output_size : 出力サイズ（MNISTの場合は10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
    "        'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
    "        'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"損失関数を求める\n",
    "        引数のxは入力データ、tは教師ラベル\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（数値微分）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（誤差逆伝搬法）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ce91b2-0d7a-465c-a056-050f9742e30d",
   "metadata": {},
   "source": [
    "* # MNISTデータセットに対して学習と予測を行ってみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9864409e-30c3-4200-b330-ea30f1ee5244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.298877533975644\n",
      "=== epoch:1, train acc:0.257, test acc:0.261 ===\n",
      "train loss:2.295309830897032\n",
      "train loss:2.2926989171859256\n",
      "train loss:2.2841202581873197\n",
      "train loss:2.276343338438866\n",
      "train loss:2.2419306679599416\n",
      "train loss:2.2468742791144267\n",
      "train loss:2.1872279172456723\n",
      "train loss:2.195035269637235\n",
      "train loss:2.1748456232448587\n",
      "train loss:2.1566523538070914\n",
      "train loss:2.0692576348195346\n",
      "train loss:2.078309071700958\n",
      "train loss:2.001067048436677\n",
      "train loss:1.9120166626031965\n",
      "train loss:1.983003713000328\n",
      "train loss:1.7461249182420915\n",
      "train loss:1.630739660656441\n",
      "train loss:1.6429634076810877\n",
      "train loss:1.5965574808933727\n",
      "train loss:1.5690923974589233\n",
      "train loss:1.4671596312295776\n",
      "train loss:1.296497814324545\n",
      "train loss:1.1654405255037172\n",
      "train loss:1.2009044120087558\n",
      "train loss:1.0858464424579939\n",
      "train loss:1.0260094331393008\n",
      "train loss:0.9789876575134757\n",
      "train loss:0.9400266885508417\n",
      "train loss:0.9119501240415686\n",
      "train loss:0.839389719067243\n",
      "train loss:0.8040769113712907\n",
      "train loss:0.7451680065252804\n",
      "train loss:0.7300514652163079\n",
      "train loss:0.909711204731679\n",
      "train loss:0.5471475708331418\n",
      "train loss:0.7193672595778402\n",
      "train loss:0.7819538734345735\n",
      "train loss:0.705292035838732\n",
      "train loss:0.633668279723581\n",
      "train loss:0.7889732678996182\n",
      "train loss:0.5563271058928363\n",
      "train loss:0.6873050006596327\n",
      "train loss:0.6207449780730095\n",
      "train loss:0.5084106413057788\n",
      "train loss:0.5325581974066814\n",
      "train loss:0.47741386092313803\n",
      "train loss:0.6283346721193541\n",
      "train loss:0.7475721463721858\n",
      "train loss:0.6610363633914066\n",
      "train loss:0.6946683386317457\n",
      "train loss:0.6018442004404199\n",
      "train loss:0.5884523794830488\n",
      "train loss:0.5608180880437456\n",
      "train loss:0.4978426931998801\n",
      "train loss:0.40350428186316334\n",
      "train loss:0.5287044229941565\n",
      "train loss:0.43580941143161894\n",
      "train loss:0.42179865076867545\n",
      "train loss:0.5299555758398418\n",
      "train loss:0.42844498515380636\n",
      "train loss:0.6271909636459846\n",
      "train loss:0.3201701726889468\n",
      "train loss:0.5506848118642487\n",
      "train loss:0.5406103393245308\n",
      "train loss:0.35766592440979006\n",
      "train loss:0.5017105059545452\n",
      "train loss:0.5499758792357031\n",
      "train loss:0.5408160898834243\n",
      "train loss:0.5003963398562078\n",
      "train loss:0.42078399772067704\n",
      "train loss:0.487766151829087\n",
      "train loss:0.508224044429953\n",
      "train loss:0.45092489562742855\n",
      "train loss:0.527789565890701\n",
      "train loss:0.4130322591808141\n",
      "train loss:0.4288911929695747\n",
      "train loss:0.43552138954322595\n",
      "train loss:0.5017725788733852\n",
      "train loss:0.5477336606993843\n",
      "train loss:0.33995404106362104\n",
      "train loss:0.3752198341255737\n",
      "train loss:0.6304839851048899\n",
      "train loss:0.46693532512595437\n",
      "train loss:0.47946646832983675\n",
      "train loss:0.3533074825686496\n",
      "train loss:0.4103038225069614\n",
      "train loss:0.3320293988244113\n",
      "train loss:0.4448118620220645\n",
      "train loss:0.273261355185664\n",
      "train loss:0.3483120644044456\n",
      "train loss:0.5538542360281306\n",
      "train loss:0.35279901481085657\n",
      "train loss:0.37393619224837854\n",
      "train loss:0.4691915010392638\n",
      "train loss:0.4084593593835546\n",
      "train loss:0.4281730810819233\n",
      "train loss:0.3565322453210669\n",
      "train loss:0.6627946129866402\n",
      "train loss:0.28099471186134894\n",
      "train loss:0.2825724630696725\n",
      "train loss:0.305194957837786\n",
      "train loss:0.5068034804781684\n",
      "train loss:0.6121000663097781\n",
      "train loss:0.4624960477393828\n",
      "train loss:0.30258188047605206\n",
      "train loss:0.41216316056904667\n",
      "train loss:0.3832384514689236\n",
      "train loss:0.3429314031064247\n",
      "train loss:0.4132968824254955\n",
      "train loss:0.3594932614045284\n",
      "train loss:0.32245906436601646\n",
      "train loss:0.334044149779202\n",
      "train loss:0.29151255835485773\n",
      "train loss:0.28671277062608513\n",
      "train loss:0.4725597070238286\n",
      "train loss:0.4785978087078822\n",
      "train loss:0.735834189621943\n",
      "train loss:0.39410241126935863\n",
      "train loss:0.4956474988330825\n",
      "train loss:0.48497771316327176\n",
      "train loss:0.32917790651719164\n",
      "train loss:0.20731487088503242\n",
      "train loss:0.4977077269683422\n",
      "train loss:0.330475230938893\n",
      "train loss:0.3291791505930029\n",
      "train loss:0.42261913146268654\n",
      "train loss:0.2588138006740327\n",
      "train loss:0.3752119723270297\n",
      "train loss:0.40159982545975526\n",
      "train loss:0.42551197719763073\n",
      "train loss:0.36902172204778183\n",
      "train loss:0.4784876148493575\n",
      "train loss:0.2649579146025427\n",
      "train loss:0.43203464122177837\n",
      "train loss:0.3340609669938901\n",
      "train loss:0.24247922857137\n",
      "train loss:0.3954239155969453\n",
      "train loss:0.30912025449639663\n",
      "train loss:0.29779940019202805\n",
      "train loss:0.30624901795457193\n",
      "train loss:0.3593665555210402\n",
      "train loss:0.3153214479276725\n",
      "train loss:0.42690255008540423\n",
      "train loss:0.23917276858069111\n",
      "train loss:0.3387094559751084\n",
      "train loss:0.3192572888408865\n",
      "train loss:0.23434804134024462\n",
      "train loss:0.27554154721919666\n",
      "train loss:0.45456017006249894\n",
      "train loss:0.21346361894148802\n",
      "train loss:0.30051568602551\n",
      "train loss:0.24715139314273382\n",
      "train loss:0.25244366289791165\n",
      "train loss:0.46203058759775567\n",
      "train loss:0.32593427099454786\n",
      "train loss:0.4243395160709542\n",
      "train loss:0.2807101580907312\n",
      "train loss:0.2567161427329526\n",
      "train loss:0.22582784823811689\n",
      "train loss:0.28066455537406315\n",
      "train loss:0.3454456251003986\n",
      "train loss:0.49345751935424054\n",
      "train loss:0.3217997109552417\n",
      "train loss:0.3574459822883254\n",
      "train loss:0.36396234425841223\n",
      "train loss:0.3347195847309123\n",
      "train loss:0.21318660525229824\n",
      "train loss:0.42393292294642654\n",
      "train loss:0.36545990315089555\n",
      "train loss:0.32735914791667786\n",
      "train loss:0.39646249047024645\n",
      "train loss:0.36767815728350756\n",
      "train loss:0.365248568155545\n",
      "train loss:0.32611769244090694\n",
      "train loss:0.24783624865730136\n",
      "train loss:0.26838905306394684\n",
      "train loss:0.256205119382409\n",
      "train loss:0.17839674392667462\n",
      "train loss:0.3934530154726279\n",
      "train loss:0.27368764449751876\n",
      "train loss:0.2999508154501274\n",
      "train loss:0.2878604008535552\n",
      "train loss:0.22424361896303477\n",
      "train loss:0.22255279055415994\n",
      "train loss:0.29928976381840755\n",
      "train loss:0.2543323549849648\n",
      "train loss:0.2301382330383059\n",
      "train loss:0.1850284913752309\n",
      "train loss:0.15460989607309003\n",
      "train loss:0.33895659191169014\n",
      "train loss:0.3503883490654246\n",
      "train loss:0.3634759239693354\n",
      "train loss:0.1725346984085188\n",
      "train loss:0.2189482308575778\n",
      "train loss:0.23007852315115015\n",
      "train loss:0.21051313221319184\n",
      "train loss:0.16066978121488412\n",
      "train loss:0.2631253565900301\n",
      "train loss:0.1568569279092097\n",
      "train loss:0.5099842383786745\n",
      "train loss:0.3451492321705309\n",
      "train loss:0.3414453169992819\n",
      "train loss:0.20890179891686908\n",
      "train loss:0.3299047633924652\n",
      "train loss:0.2584404702550591\n",
      "train loss:0.32407323785319264\n",
      "train loss:0.19773373323671556\n",
      "train loss:0.2616004324623148\n",
      "train loss:0.37143677884790677\n",
      "train loss:0.22503155872289982\n",
      "train loss:0.3209366384231916\n",
      "train loss:0.34583866746353614\n",
      "train loss:0.2824106233704434\n",
      "train loss:0.29495343791608714\n",
      "train loss:0.45836074727172454\n",
      "train loss:0.36111913709749965\n",
      "train loss:0.20795019126298475\n",
      "train loss:0.3125316563497152\n",
      "train loss:0.2661745352650241\n",
      "train loss:0.25229271185761076\n",
      "train loss:0.20264584849925854\n",
      "train loss:0.2537915463436078\n",
      "train loss:0.19982360895306578\n",
      "train loss:0.4164277453998207\n",
      "train loss:0.3429431268269604\n",
      "train loss:0.22640961367237494\n",
      "train loss:0.3589215563186737\n",
      "train loss:0.19959242334041224\n",
      "train loss:0.24364584274592502\n",
      "train loss:0.5300925621829351\n",
      "train loss:0.27201552121005806\n",
      "train loss:0.31203310925412986\n",
      "train loss:0.33543815094816204\n",
      "train loss:0.1235892619933589\n",
      "train loss:0.27873433058971914\n",
      "train loss:0.34641393539385623\n",
      "train loss:0.33315614381090486\n",
      "train loss:0.18605155293492262\n",
      "train loss:0.43940952126139315\n",
      "train loss:0.32405842758380204\n",
      "train loss:0.3455525219229307\n",
      "train loss:0.1156716547898337\n",
      "train loss:0.2542783646160178\n",
      "train loss:0.3251407873321059\n",
      "train loss:0.13392799646865516\n",
      "train loss:0.2500214186065178\n",
      "train loss:0.15579923193628734\n",
      "train loss:0.3553733109087037\n",
      "train loss:0.3236300335684227\n",
      "train loss:0.11854419281607109\n",
      "train loss:0.20820452315033325\n",
      "train loss:0.22406419179986364\n",
      "train loss:0.1952031719057799\n",
      "train loss:0.2582788100148649\n",
      "train loss:0.15819643334951894\n",
      "train loss:0.2666359211130747\n",
      "train loss:0.25039548049144633\n",
      "train loss:0.176592724234596\n",
      "train loss:0.21802887126577075\n",
      "train loss:0.3733114854310004\n",
      "train loss:0.2771165438224383\n",
      "train loss:0.3230806538708522\n",
      "train loss:0.3026811339595856\n",
      "train loss:0.19562357368963543\n",
      "train loss:0.25568567297315925\n",
      "train loss:0.3385873798411238\n",
      "train loss:0.21002520401846675\n",
      "train loss:0.15894192630684498\n",
      "train loss:0.2364709559035139\n",
      "train loss:0.22852366507000244\n",
      "train loss:0.1985729956903334\n",
      "train loss:0.2940935675377538\n",
      "train loss:0.19703392637255576\n",
      "train loss:0.28456570095558503\n",
      "train loss:0.287573178332281\n",
      "train loss:0.3293631727983747\n",
      "train loss:0.3248479830865416\n",
      "train loss:0.29758278346809613\n",
      "train loss:0.2505309179460533\n",
      "train loss:0.32199512936977975\n",
      "train loss:0.3589939227190012\n",
      "train loss:0.15867158672976317\n",
      "train loss:0.22071104944293993\n",
      "train loss:0.2382312725558703\n",
      "train loss:0.19816834857456134\n",
      "train loss:0.19958356939581892\n",
      "train loss:0.22910538983951734\n",
      "train loss:0.20146023910622302\n",
      "train loss:0.21277695604406563\n",
      "train loss:0.25540295849757955\n",
      "train loss:0.2016873731465666\n",
      "train loss:0.1853423222153776\n",
      "train loss:0.1013596230221964\n",
      "train loss:0.1626079927306964\n",
      "train loss:0.22894761578977899\n",
      "train loss:0.21977605153010554\n",
      "train loss:0.29456525823751734\n",
      "train loss:0.265180891634108\n",
      "train loss:0.1808072665464568\n",
      "train loss:0.2371769027900993\n",
      "train loss:0.2550521562641091\n",
      "train loss:0.17690084614472398\n",
      "train loss:0.17803500827197297\n",
      "train loss:0.17595401167323238\n",
      "train loss:0.23459228882582062\n",
      "train loss:0.21658986778109773\n",
      "train loss:0.1991842121991256\n",
      "train loss:0.16717906178932584\n",
      "train loss:0.22805080495152563\n",
      "train loss:0.31481055943837355\n",
      "train loss:0.16186486424245733\n",
      "train loss:0.23373901994130775\n",
      "train loss:0.20797041011527778\n",
      "train loss:0.3321352127236068\n",
      "train loss:0.1198759315957403\n",
      "train loss:0.12795716667554013\n",
      "train loss:0.22585143006949604\n",
      "train loss:0.25281357500896756\n",
      "train loss:0.3771602208893324\n",
      "train loss:0.36494556243101606\n",
      "train loss:0.2702344750878832\n",
      "train loss:0.2764802963434022\n",
      "train loss:0.14214901339256478\n",
      "train loss:0.3363751490145212\n",
      "train loss:0.23022586521368674\n",
      "train loss:0.3885251259581684\n",
      "train loss:0.34233426026586344\n",
      "train loss:0.15298662893649162\n",
      "train loss:0.19300746391947865\n",
      "train loss:0.15438595482764153\n",
      "train loss:0.12265257664199751\n",
      "train loss:0.1381459089322264\n",
      "train loss:0.35649893659790516\n",
      "train loss:0.12875436118490463\n",
      "train loss:0.19568688411072968\n",
      "train loss:0.14993083121161835\n",
      "train loss:0.17720595776590295\n",
      "train loss:0.29882143954199536\n",
      "train loss:0.19150953870175433\n",
      "train loss:0.2963332790422829\n",
      "train loss:0.2109128571807252\n",
      "train loss:0.24669918643687513\n",
      "train loss:0.24339181767543494\n",
      "train loss:0.2499771146420727\n",
      "train loss:0.16433745099014896\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e4df5-a92a-48a6-af68-9b80e56aa5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
